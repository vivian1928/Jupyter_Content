{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be301c4-61af-4f40-be5f-57f2ba4b7733",
   "metadata": {},
   "source": [
    "# Natural Language Processing Intro\n",
    "\n",
    "Natural Language Processing (NLP) is a large field of AI with many related, but somewhat distinct sub-disciplines. We won't have time to look at all of these, but you are most likely somewhat familiar with many of the applications.\n",
    "\n",
    "Some NLP Tasks:\n",
    "* Summary generation, information extractions\n",
    "* Translation (language to language)\n",
    "* Transcription (speech to text)\n",
    "* Auto-completion\n",
    "* Sentiment Analysis\n",
    "* Intent Detection\n",
    "* Chat, automated writing, dialog generation, question answering\n",
    "* Voice assistant\n",
    "* Document retrieval\n",
    "\n",
    "## Ambiguity in language\n",
    "\n",
    "NLP is not a simple task, and until deep learning, was quite limited in its abilities, though as with most fields in AI, there is a long history, [dating back to the 1950s](https://en.wikipedia.org/wiki/Natural_language_processing). Part of the challenge is that human language tends to be ambiguous and recognizing words is really only the start to inferring meaning. Take for example this sentence:\n",
    "   > The boy saw a man with a telescope\n",
    "   \n",
    "   * Who had the telescope?\n",
    "   \n",
    "More context is needed to answer this. Yet, context is not always there and even when it is, can be a challenge for NLP methods.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "One of the primary challenges of NLP is representing language as numbers--remember computers, and the ML/AI systems we have, primarily deal with numbers. For computer vision problems, this was relatively easy in that we took pixel intensities of an image and fed those in. But what to do with speech, words, text?\n",
    "\n",
    "The processes of converting text to numerical representation is called **tokenization**. There are many methods of tokenization, but the idea is to break text into itemizable components--tokens.\n",
    "\n",
    "Tokens can be words, letters, word fragments, or even sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda4b88-5387-4814-9748-24f17dabc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the examples here are adapted from https://github.com/NVDLI/LDL\n",
    "# Which supports the book Learning Deep Learning (LDL) by Magnus Ekman (ISBN: 9780137470358).\n",
    "\n",
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "Copyright (c) 2021 NVIDIA\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784964dc-9e92-4314-add7-4341a8e79f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text \\\n",
    "    import text_to_word_sequence\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 10  # originally 32, reduced for time.\n",
    "BATCH_SIZE = 256\n",
    "INPUT_FILE_NAME = 'data/frankenstein.txt'\n",
    "WINDOW_LENGTH = 40\n",
    "WINDOW_STEP = 3\n",
    "PREDICT_LENGTH = 3\n",
    "MAX_WORDS = 10000\n",
    "EMBEDDING_WIDTH = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c20dc-65b3-4133-93ba-94dfed96b89e",
   "metadata": {},
   "source": [
    "## Cleaning, tokenization and creation of training set\n",
    "\n",
    "In the following block, we read in the [*Frankenstein*](https://www.gutenberg.org/ebooks/84) text file and use the `text_to_word_sequence` to convert the text to a list of individual words. This also removes punctuation and converts everything to lower case. This command accomplishes our cleaning and tokenization steps.\n",
    "\n",
    "The next step is to create the training set of `fragments` and corresponding `targets`. The hyperparameters were set above, and used here to make a training set by sliding a window over the text, `WINDOW_LENGTH=40` words at a time. \n",
    "\n",
    "The word immediately after that becomes the `target` and the window shifts down `WINDOW_STEP=3` words to make the next fragment/target pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace86526-a34a-4790-acd9-7ba7e266b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read file.\n",
    "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8-sig')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# Make lower case and split into individual words.\n",
    "text = text_to_word_sequence(text)\n",
    "\n",
    "# Create training examples.\n",
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
    "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
    "    targets.append(text[i + WINDOW_LENGTH])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22d67e-d404-47b5-947f-838416c978ea",
   "metadata": {},
   "source": [
    "### Look at one fragment/target pair\n",
    "\n",
    "The following text from the book happens to be *fragment*/**target** pair 94:\n",
    " > *I am already far north of London, and as I walk in the streets of\n",
    "Petersburgh, I feel a cold northern breeze play upon my cheeks, which\n",
    "braces my nerves and fills me with delight. Do you understand* **this**\n",
    "feeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38b43c-8b9e-481a-bae8-1ea8d7dd1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Fragment: {fragments[94]}\\nTarget: {targets[94]}')\n",
    "\n",
    "print(f'\\nTotal fragments: {len(fragments)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef736ba-456f-4094-9229-b09eb77dba47",
   "metadata": {},
   "source": [
    "So, now we have the whole text split into 26,238 fragments, each with the following word as the target. This is what our model will be trained on.\n",
    "\n",
    "But we have another step before we can begin training.\n",
    "\n",
    "## Embedding\n",
    "\n",
    "Somehow we need to convert the words (both in the fragments and the targets) to numbers. Embedding goes a step further, representing words in n-dimensional vector space. This vector space is a lower dimensionality than the number of words in the vocabulary (as opposed to one-hot encoding) and results in words with with similar meaning being closer in space than other word.\n",
    "\n",
    "Here's an example figure showing some embeddings (taken from Renu Khandelwal's article [*Word Embeddings for NLP*](https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4):\n",
    "\n",
    "\n",
    "![Image of word embeddings from https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4 ](images/word_embeddings_Renu_Khandelwal.png)\n",
    "\n",
    "The embedding is learned in the training process. In this step, we are assigning numbers to words. Above, we set `MAX_WORDS=10000` so, once we have 10,000 words in the vocabulary, all additional words will be assigned to the `UNK` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c65bda-8e86-4918-9b83-a8d5852640df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to indices.\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(text)\n",
    "fragments_indexed = tokenizer.texts_to_sequences(fragments)\n",
    "targets_indexed = tokenizer.texts_to_sequences(targets)\n",
    "\n",
    "# Convert to appropriate input and output formats.\n",
    "X = np.array(fragments_indexed, dtype=np.int64)\n",
    "y = np.zeros((len(targets_indexed), MAX_WORDS))\n",
    "for i, target_index in enumerate(targets_indexed):\n",
    "    y[i, target_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32991b-10bc-491f-8fa9-8b799b271511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Fragment as numbers: {X[94]} \\nTarget is one-hot encoded: {y[94]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d64624f-86fc-4d31-928b-b4949fdde18a",
   "metadata": {},
   "source": [
    "## Build and fit the model\n",
    "\n",
    "Our first layer is the embedding layer that learns to convert the numbered words in the vocabulary to the embedding. Then we have two LSTM layers and a dense layer with a ReLU activation and a final layer with one neuron per word in the vocabulary and a softmax to make the output the probability of each word being the output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf3e22-815f-4c91-8c1b-5f1c20d91e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train model.\n",
    "training_model = Sequential()\n",
    "training_model.add(Embedding(\n",
    "    output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS,\n",
    "    mask_zero=True, input_length=None))\n",
    "training_model.add(LSTM(128, return_sequences=True,\n",
    "                        dropout=0.2, recurrent_dropout=0.2))\n",
    "training_model.add(LSTM(128, dropout=0.2,\n",
    "                        recurrent_dropout=0.2))\n",
    "training_model.add(Dense(128, activation='relu'))\n",
    "training_model.add(Dense(MAX_WORDS, activation='softmax'))\n",
    "training_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer='adam')\n",
    "training_model.summary()\n",
    "history = training_model.fit(X, y, validation_split=0.05,\n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             epochs=EPOCHS, verbose=2, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4967c2-0654-4341-aee8-2922dd98fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stateful model used for prediction.\n",
    "inference_model = Sequential()\n",
    "inference_model.add(Embedding(\n",
    "    output_dim=EMBEDDING_WIDTH, input_dim=MAX_WORDS,\n",
    "    mask_zero=True, batch_input_shape=(1, 1)))\n",
    "inference_model.add(LSTM(128, return_sequences=True,\n",
    "                         dropout=0.2, recurrent_dropout=0.2,\n",
    "                         stateful=True))\n",
    "inference_model.add(LSTM(128, dropout=0.2,\n",
    "                         recurrent_dropout=0.2, stateful=True))\n",
    "inference_model.add(Dense(128, activation='relu'))\n",
    "inference_model.add(Dense(MAX_WORDS, activation='softmax'))\n",
    "weights = training_model.get_weights()\n",
    "inference_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146c8ad-33a5-481a-9189-df5351555a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide beginning of sentence and\n",
    "# predict next words in a greedy manner\n",
    "first_words = ['i', 'saw']\n",
    "first_words_indexed = tokenizer.texts_to_sequences(\n",
    "    first_words)\n",
    "inference_model.reset_states()\n",
    "predicted_string = ''\n",
    "\n",
    "# Feed initial words to the model.\n",
    "for i, word_index in enumerate(first_words_indexed):\n",
    "    x = np.zeros((1, 1), dtype=np.int64)\n",
    "    x[0][0] = word_index[0]\n",
    "    predicted_string += first_words[i]\n",
    "    predicted_string += ' '\n",
    "    y_predict = inference_model.predict(x, verbose=0)[0]\n",
    "\n",
    "# Predict PREDICT_LENGTH words.\n",
    "for i in range(PREDICT_LENGTH):\n",
    "    new_word_index = np.argmax(y_predict)\n",
    "    word = tokenizer.sequences_to_texts(\n",
    "        [[new_word_index]])\n",
    "    x[0][0] = new_word_index\n",
    "    predicted_string += word[0]\n",
    "    predicted_string += ' '\n",
    "    y_predict = inference_model.predict(x, verbose=0)[0]\n",
    "print(predicted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1149ed9-0b09-4a93-be31-63a39b9d0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict more by changing predict length to 10\n",
    "\n",
    "PREDICT_LENGTH = 10\n",
    "\n",
    "# Predict PREDICT_LENGTH words.\n",
    "for i in range(PREDICT_LENGTH):\n",
    "    new_word_index = np.argmax(y_predict)\n",
    "    word = tokenizer.sequences_to_texts(\n",
    "        [[new_word_index]])\n",
    "    x[0][0] = new_word_index\n",
    "    predicted_string += word[0]\n",
    "    predicted_string += ' '\n",
    "    y_predict = inference_model.predict(x, verbose=0)[0]\n",
    "print(predicted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f78ebc-5fd0-46b7-bed8-e873f1897fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore embedding similarities.\n",
    "embeddings = training_model.layers[0].get_weights()[0]\n",
    "lookup_words = ['the', 'saw', 'see', 'of', 'and',\n",
    "                'monster', 'frankenstein', 'read', 'eat']\n",
    "for lookup_word in lookup_words:\n",
    "    lookup_word_indexed = tokenizer.texts_to_sequences(\n",
    "        [lookup_word])\n",
    "    print('words close to:', lookup_word)\n",
    "    lookup_embedding = embeddings[lookup_word_indexed[0]]\n",
    "    word_indices = {}\n",
    "    # Calculate distances.\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        distance = np.linalg.norm(\n",
    "            embedding - lookup_embedding)\n",
    "        word_indices[distance] = i\n",
    "    # Print sorted by distance.\n",
    "    for distance in sorted(word_indices.keys())[:5]:\n",
    "        word_index = word_indices[distance]\n",
    "        word = tokenizer.sequences_to_texts([[word_index]])[0]\n",
    "        print(word + ': ', distance)\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
