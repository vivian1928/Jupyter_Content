{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9e9e85-e6ad-4e3e-a696-e329aeb8f60c",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "<div class=\"alert  alert-warning\" role=\"alert\">\n",
    "    Note this notebook should be run using the <strong>nlp-1.2 kernel</strong> on HiPerGator.\n",
    "</div>\n",
    "    \n",
    "The goal of this notebook is to try to make the links from NLP to the emergence of transformer architectures in NLP to set us up for looking at broader applications of transformers beyond NLP. I'm trying to not get bogged down in the details of NLP or transformers, but provide some intuition on how transformers revolutionized NLP and how they are now moving on to impact other fields.\n",
    "\n",
    "In his [GTC 2022 Keynote](https://www.nvidia.com/gtc/keynote/), Nvidia's CEO, Jensen  Huang, stated:\n",
    "> The Transformer is unquestionably the most important deep learning model invented.\n",
    "\n",
    "That says a lot for an architecture introduced only 5-years earlier in 2017!\n",
    "\n",
    "This notebook largely follows, and quotes from, the [Hugging Face Transformer Course](https://huggingface.co/course/chapter1/3?fw=tf). It starts with some motivating examples.\n",
    "\n",
    "Here are some examples of what Transformers can do in NLP:\n",
    "\n",
    "## Set up Cache directory\n",
    "\n",
    "By default, the Huggingface `transformers` module will download things to `~/.chache/huggingface`. There are a couple of problems with that. First, HiPerGator home directories are limited to 40GB and these files can add up fast. Second, it can take a while to download the first time you run. So, I've downloaded everything for this notebook and copied the cache to `/blue/zoo4926/share/huggingface/`. Let's link your cache to there.\n",
    "\n",
    "Note...this might not be the best method. But I ran out of time and the documentation didn't seem to match actual behavior for changing the default path... I'll provide the cell to undo the change too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc94e94-6d61-404a-beb4-2314da8e456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a symbolic link in your ~/.cache folder to /blue/zoo4926/share/huggingface/\n",
    "\n",
    "!ln -s /blue/zoo4926/share/huggingface/ ~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c03611-2b40-44aa-96f5-ff2fd5e52485",
   "metadata": {},
   "source": [
    "**ONLY RUN THE FOLLOWING CELL WHEN YOU ARE DONE WITH THE NOTEBOOK**\n",
    "\n",
    "When you are done with this notebook, you can remove this link.\n",
    "\n",
    "Uncomment (remove the #) the line below an drun the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45722b71-4b72-496a-a602-c2337eec52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run when done with this notebook.\n",
    "#!rm ~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc524a82-9800-4ba9-91d8-ea979fdb7044",
   "metadata": {},
   "source": [
    "## Some Examples\n",
    "\n",
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b6f9b6-f59b-4736-8862-f60fc6e3ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd60204-e7e6-4f82-9c6b-4cca96367ffc",
   "metadata": {},
   "source": [
    "> By default, this pipeline selects a particular pre-trained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the `classifier` object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
    ">\n",
    "> There are three main steps involved when you pass some text to a pipeline:\n",
    ">\n",
    "> 1. The text is preprocessed into a format the model can understand.\n",
    "> 1. The preprocessed inputs are passed to the model.\n",
    "> 1. The predictions of the model are post-processed, so you can make sense of them.\n",
    ">\n",
    "> Some of the currently [available pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) are:\n",
    "> \n",
    "> * `feature-extraction` (get the vector representation of a text)\n",
    "> * `fill-mask`\n",
    "> * `ner` (named entity recognition)\n",
    "> * `question-answering`\n",
    "> * `sentiment-analysis`\n",
    "> * `summarization`\n",
    "> * `text-generation`\n",
    "> * `translation`\n",
    "> * `zero-shot-classification`\n",
    ">\n",
    ">Let’s have a look at a few of these!\n",
    "\n",
    "### Zero-shot classification\n",
    "\n",
    "> We’ll start by tackling a more challenging task where we need to classify texts that haven’t been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the zero-shot-classification pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don’t have to rely on the labels of the pre-trained model. You’ve already seen how the model can classify a sentence as positive or negative using those two labels — but it can also classify the text using any other set of labels you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d583a893-0300-49b8-89b7-eba4d68f7855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163351fb838a4509b6c56f3e369f85ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1018fa09766b45b3bbb8b74f7a0375ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c7be7b4b2e46358881710fea608094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f67f8693c04b26bab89e093f03fbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371e11dc05be44158506c6e0c143370b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3349eb752fe4556ab4391f2f44a6d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445965647697449, 0.11197599768638611, 0.04342741146683693]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22f804-2662-4bc9-ada6-07bf07f2b8e2",
   "metadata": {},
   "source": [
    "> This pipeline is called zero-shot because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!\n",
    "\n",
    "### Text Generation\n",
    "\n",
    "> Now let’s see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it’s normal if you don’t get the same results as shown below.\n",
    "\n",
    "**Note** The default `gpt2` model does not seem to work on HiPerGator at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de613df-9ca1-437b-b70b-cda8fc2cfde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to learn more skills of communication in writing. Topics include: Communication and writing writing\\n\\n\\n\\n\\n'},\n",
       " {'generated_text': 'In this course, we will teach you how to build an efficient web server based on your users base and your user base. The first part will show'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9123d-4bf4-449b-aed3-dfdba7e34571",
   "metadata": {},
   "source": [
    "### Mask Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa906cf-d404-4f53-b92b-cf563773b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b249b63276947ca90f845c2ce57d83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcff0baecab4b0fb23876c5c4e79def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6547cb96778f4e7cab0e80539796d5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fc4615659542aa9a1e3393aade3404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e995165f0d76428a9debff70598a2c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This course will teach you all about mathematical models.',\n",
       "  'score': 0.1961984932422638,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'This course will teach you all about computational models.',\n",
       "  'score': 0.04052729159593582,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c4d09-1813-4a7a-ad07-d3ffd75a40f1",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e90aba8e-c9da-4f13-976c-742b9975f8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "/apps/nlp/1.2/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:128: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99930125,\n",
       "  'word': 'Matt',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.997988,\n",
       "  'word': 'University of Florida',\n",
       "  'start': 34,\n",
       "  'end': 55},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9874652,\n",
       "  'word': 'Gainesville',\n",
       "  'start': 59,\n",
       "  'end': 70}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Matt and I work at the University of Florida in Gainesville.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c309d54b-4b15-40d6-9503-1fb83fe5089d",
   "metadata": {},
   "source": [
    "### Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f75bba8-7733-41f2-b597-98370ae633ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.4767897427082062,\n",
       " 'start': 34,\n",
       " 'end': 55,\n",
       " 'answer': 'University of Florida'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Matt and I work at the University of Florida in Gainesville.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eff250-b146-4167-a623-8ae5b89d990b",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad99d04-0927-4f66-b816-768c8dd10ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c6c50-f366-4cd2-8432-c559bbee6b54",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f197859-2880-477c-8572-60df2459abd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3817f88ee111468a8ec7320334fbd5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f231b827dd4493bdea1c0248ff7a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/287M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0004bdb9f21b40848bbace26d60d6024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9574bd9a5f48338366cfefc937fe2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/784k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e3431e07604922b44e056d24f58606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ecd68a7f544cf89cffa6ead26050bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06745c71-46bf-4cb3-bc28-2b022932aa7b",
   "metadata": {},
   "source": [
    "## Tranformers transform NLP and more!\n",
    "\n",
    "Transformers are not that old! Only having been introduced in 2017! Vaswani *et al.*'s paper [*Attention is all you need*](https://arxiv.org/abs/1706.03762) has over 38,000 citations in Google Scholar (3/22/2022)\n",
    "\n",
    "![Transformer timeline from Hugging Face](images/transformers_chrono_HuggingFace.png)\n",
    "\n",
    "Since its publication, transformers have revolutionized NLP and been applied to other fields, including computer vision.\n",
    "\n",
    "### Transformers are language models\n",
    "\n",
    "> All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as language models. This means they have been trained on large amounts of raw text in a self-supervised fashion. **Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!**\n",
    "> \n",
    "> This type of model develops a statistical understanding of the language it has been trained on, but it’s not very useful for specific practical tasks. Because of this, the general pre-trained model then goes through a process called transfer learning. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
    "\n",
    "### Transformers are **BIG** models\n",
    "\n",
    "The graph below is from the article [*We Might See A 100T Language Model In 2022*](https://analyticsindiamag.com/we-might-see-a-100t-language-model-in-2022/), published in Dec 2021:\n",
    "![Image of transformer model size over time, from https://analyticsindiamag.com/we-might-see-a-100t-language-model-in-2022/](images/NVIDIA_NLP_Model_Size.png)\n",
    "\n",
    "And costly in terms of compute and CO2 emissions...\n",
    "\n",
    "![Relative CO2 emissions for a variety of human activities, from Hugging Face](images/carbon_footprint_HuggingFace.png)\n",
    "\n",
    "Luckily, as we've seen, transfer learning can help!!\n",
    "\n",
    "![Schematic of pre-training from Hugging Face](images/pretraining_HuggingFace.png)\n",
    "\n",
    "> This pre-training is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks.\n",
    ">\n",
    "> *Fine-tuning*, on the other hand, is the training done **after** a model has been pre-trained. To perform fine-tuning, you first acquire a pre-trained language model, then perform additional training with a dataset specific to your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56410ead-8669-4c8e-b543-155f20025030",
   "metadata": {},
   "source": [
    "## Transformer architecture\n",
    "\n",
    "> The model is primarily composed of two blocks:\n",
    "> * **Encoder (left)**: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
    "> * **Decoder (right)**: The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.\n",
    "\n",
    "![Basic transformer architecture, from Hugging Face](images/transformers_blocks_HuggingFace.png)\n",
    "\n",
    "> Each of these parts can be used independently, depending on the task:\n",
    "> * **Encoder-only models**: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
    "> * **Decoder-only models**: Good for generative tasks such as text generation.\n",
    "> * **Encoder-decoder models** or **sequence-to-sequence models**: Good for generative tasks that require an input, such as translation or summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc7ea7-b278-4d1d-8ca1-7d0e82a9f1b0",
   "metadata": {},
   "source": [
    "## Attention layers\n",
    "\n",
    "> A key feature of Transformer models is that they are built with special layers called attention layers. In fact, the title of the paper introducing the Transformer architecture was [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762)! ...for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.\n",
    ">\n",
    "> To put this into context, consider the task of translating text from English to French. Given the input “You like this course”, a translation model will need to also attend to the adjacent word “You” to get the proper translation for the word “like”, because in French the verb “like” is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating “this” the model will also need to pay attention to the word “course”, because “this” translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of “this”. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word.\n",
    ">\n",
    "> The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied.\n",
    "> \n",
    "> Now that you have an idea of what attention layers are all about, let’s take a closer look at the Transformer architecture.\n",
    "\n",
    "## The original architecture\n",
    "\n",
    "> The Transformer architecture was originally designed for translation. During training, the **encoder** receives inputs (sentences) in a certain language, while the **decoder** receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word.\n",
    ">\n",
    "> To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3.\n",
    "> \n",
    "> The original Transformer architecture looked like this, with the encoder on the left and the decoder on the right:\n",
    "\n",
    "![Architecture of a Transformers models](images/transformers.png)\n",
    "\n",
    "> Note that the the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word.\n",
    "> \n",
    "> The attention mask can also be used in the encoder/decoder to prevent the model from paying attention to some special words — for instance, the special padding word used to make all the inputs the same length when batching together sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9deec72-873e-427d-81de-4d448c3020ca",
   "metadata": {},
   "source": [
    "## Biases and Limiatations of NLP Models\n",
    "\n",
    "> If your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.\n",
    ">\n",
    "> To give a quick illustration, let’s go back the example of a fill-mask pipeline with the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54412caa-fbfb-4d00-a370-12eae4a80180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dab4351382c42b7832da07fe4df51ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842639732e6b4db1a8ec215c9c9b2f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f217c883ea284f7da7b9c815bb4c083e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eddc8fb329648ca93bd2fd128a6c011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9cd0a88cda47c3bfbef2ac9cab8362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb707a-c7d1-4a94-b41d-289e2c599e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-1.2",
   "language": "python",
   "name": "nlp-1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
