{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import plotnine as pn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data\n",
    "\n",
    "For this lesson, we'll work with a new dataset of unknown origin (for the moment).  Let's load the data and have a look.  Our goal is to build a model that can predict $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('regularization.csv')\n",
    "#df = pd.read_csv('/blue/zoo4926/share/Jupyter_Content/data/regularization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we do with this? We have 20 $x$ variables and 50 observations of those variables. How can we build a good predictive model for these data?\n",
    "\n",
    "## 2. Multiple linear regression\n",
    "\n",
    "Let's explore multiple linear regression first to see what happens. _Note that, in the interest of time, I am leaving out exploratory data analyses and visualizations, which is where you should start!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a model, let's use 5-fold cross-validation to estimate how well it will work for new (i.e., non-training) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "\n",
    "res = cross_val_score(model, x, y, cv=kf, scoring=mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lasso regularization\n",
    "\n",
    "Okay, so let's see what the lasso ($\\ell_1$-regularized regression) does with these data.  We can use scikit-learn's `Lasso` estimator for this.  Note that scikit-learn uses `alpha` for the name of the parameter referred to as $\\lambda$ virtually everywhere else (perhaps because `lambda` is a reserved word in Python).\n",
    "\n",
    "As a general practice, you should always make sure that your predictor variables are on the same scale when fitting a regularized regression model.  We can have `Lasso` do this for us automatically by passing `normalize=True` when we create the `Lasso` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge regression\n",
    "\n",
    "Now let's see what happens if we use ridge regression ($\\ell_2$-regularized regression) with these data.  We can use scikit-learn's `Ridge` estimator to do this, and again, don't forget to make sure that your predictor variables are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How does $\\lambda$ influence the parameter estimates?\n",
    "\n",
    "Before we move on, let's take a closer look at how changing lambda influences the parameter estimates for both the lasso and ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Choosing $\\lambda$ and a modeling procedure\n",
    "\n",
    "How should we choose the \"best\" value of $\\lambda$ for the lasso or ridge regression?  How should we decide whether to use the lasso or ridge regression?  What ideas do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Why/how do these methods work? Is one better than the other?\n",
    "\n",
    "We'll start by revealing where the data we've been modeling actually came from, which will give us some insight into how these methods work.  Then we'll dig a little deeper into how these methods are able to decrease the expected test error.\n",
    "\n",
    "Key points to remember:\n",
    "* The lasso can drive coefficients to 0; thus, i can perform _feature selection_.\n",
    "* Ridge regression constrains coefficient values (_shrinks_ them), but does not remove them from the model.\n",
    "\n",
    "Which is best depends on your data and the goals of the analysis.\n",
    "\n",
    "## 8. Other methods\n",
    "\n",
    "There are many other methods available for model/feature selection.  If you've had a statistics course, you've no doubt learned some of them.\n",
    "\n",
    "Unfortunately, we don't have time to cover any of these alternative methods in detail in this course.  I will mention, though, that _dimensionality reduction_ is another widely used, general approach to dealing with model complexity and certainly worth learning about.  _Principle components regression_ is one example of this approach.\n",
    "\n",
    "Another method I should mention is the _elastic net_, which combines $\\ell_1$ and $\\ell_2$ penalties into a single fitting procedure and can benefit from the desirable properties of both the lasso and ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data analysis lab\n",
    "\n",
    "Now, put together everything that you've learned over the last few units (prediction error, bias and variance; methods for estimating test/prediction error; regularization techniques) and apply it to a data analysis problem.  Returning to the fuel efficiency dataset, your goal is to create a machine learning model that can accurately predict the fuel efficiency of a vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/auto_mpg.csv')\n",
    "#df = pd.read_csv('/blue/zoo4926/share/Jupyter_Content/data/auto_mpg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
