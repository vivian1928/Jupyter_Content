{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating test error\n",
    "\n",
    "As we have seen, _training error_ is often a poor indicator of how well a machine learning model will perform when confronted with new data.  Today, we will look at the problem of estimating the _test error_ of a model.  Estimating test error is critical for building machine learning models that give us reliable predictions on new input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import pandas as pd\n",
    "from numpy.random import default_rng\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "\n",
    "Let's start by loading a dataset that comprises 100 observations of the function we investigated in detail during last class period:\n",
    "\n",
    "$$ y = \\frac{1}{2} \\cos(3\\pi x) + \\frac{3\\pi x}{5} + \\epsilon ,$$\n",
    "\n",
    "where $Var[\\epsilon] = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weird_function_data.csv')\n",
    "#df = pd.read_csv('/blue/zoo4926/share/Jupyter_Content/data/weird_function_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect the training error\n",
    "\n",
    "Let's fit polynomials of various degree to the full dataset and look at how the training error, as defined by $MSE$, changes.  Remember,\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2 .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge:** Implement a function to calculate $MSE$, given a model, an $x$ matrix, and an array $y$ of \"true\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The validation set method\n",
    "\n",
    "One way to estimate test error is to randomly split the training dataset into two parts, designating one part as the _training set_ and the other part as the _validation set_.  We train our model on the training set and use the validation set to estimate the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do you see any problems with this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. $k$-fold Cross-validation\n",
    "\n",
    "_$k$-fold cross-validation_ is a technique that overcomes some of the problems with using a single train/validation split.  The idea is that we randomly split our dataset into $k$ (approximately) equal-sized subsets.  We use each subset as the validation set in turn, yielding a total of $k$ estimates of our test error, which we then average to obtain our final test error estimate.  (Note that the way I am using our custom $MSE$ function in `cross_val_score()` is a little bit of a hack.  See the [scki-kit learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for the approved way of doing this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "res = cross_val_score(model, x, y, cv=kf, scoring=mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leave-one-out cross-validation\n",
    "\n",
    "What if, for $k$-fold cross-validation, we use $k = N$, where $N$ is the number of observations in our dataset?  Think for a moment about how this would work:\n",
    "  * How big will our validation sets be?\n",
    "  * How many model fits will we need?\n",
    " \n",
    "This scenario is called _leave-one-out cross-validation_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. So which is best?\n",
    "\n",
    "Short answer: bias and variance again, plus computational considerations!\n",
    "\n",
    "## 7. Final thoughts\n",
    "\n",
    "How do these methods connect to other methods for evaluating and comparing models?\n",
    "\n",
    "## 8. Practice\n",
    "\n",
    "Using the tools we discussed today, revisit the problem of predicting a car's fuel efficiency ($mpg$) from its engine's horsepower ($pw$).  Earlier, we looked at linear and quadratic models.  Would a higher-degree polynomial work better?  Don't forget to use `StandardScaler()` in your `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/auto_mpg.csv')\n",
    "#df = pd.read_csv('/blue/zoo4926/share/Jupyter_Content/data/auto_mpg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
