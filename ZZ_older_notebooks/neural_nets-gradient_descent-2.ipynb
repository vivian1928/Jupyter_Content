{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting neural nets, part 2\n",
    "\n",
    "In this notebook, we continue our journey with gradient descent.  In the previous notebook, we focused on the relatively simple case of minimizing a scalar-valued, scalar-input function.  Here, we'll take a look at how we generalize gradient descent to minimize functions of multiple variables.  This is almost always the situation when minimizing loss functions in machine learning.\n",
    "\n",
    "Again, why are we doing this?  Well, because it is interesting, but also, because _you really do need at least a basic understanding of how this works to use neural nets effectively_.\n",
    "\n",
    "You might want to have the notebook for part 1 open as a reference while you work on this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a loss function of multiple variables\n",
    "\n",
    "### 1. The model\n",
    "\n",
    "As a reminder, the equation for our simple fuel efficiency model is:\n",
    "\n",
    "$$ \\hat{mpg} = a + b \\cdot hp. $$\n",
    "\n",
    "Here, $ a $ and $ b $ are unknown constants, $ hp $ is our input, and $ \\hat{mpg} $ is our output.\n",
    "\n",
    "We can write this more generally by replacing $ hp $ with $ x $ and $ \\hat{mpg} $ with $ \\hat{y} $.\n",
    "\n",
    "$$ \\hat{y} = a + bx .$$\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, we can model this relationship using a single neuron with a single input.\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:image-2.png\" width=\"228\" />\n",
    "</div>\n",
    "\n",
    "In this case, we can write our model equation as\n",
    "\n",
    "$$ \\hat{y} = wx + b . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The loss function\n",
    "\n",
    "Recall that, when fitting a supervised machine learning model, we need a loss function to evaluate how far from \"the truth\" our model is given the model's parameter values.  For regression problems, our standard loss is the _squared error_:\n",
    "\n",
    "$$ SE = (y_0 - \\hat{y_0})^2 = (y_0 - (wx_0 + b))^2. $$\n",
    "\n",
    "Recall also that, given a set of $(x, y)$ observations, $(x_i, y_i); i = 1 ... N$ , we can calculate the average squared error over the entire dataset, which gives us the _mean squared error_:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2 = \\frac{1}{N} \\sum_{i=1}^N (y_i - (wx_i + b))^2 .$$\n",
    "\n",
    "Our goal is to find values of $w$ and $b$ that make $MSE$ as small as possible.  In this setting, MSE can be viewed _as a function of the parameters_ $w$ and $b$:\n",
    "\n",
    "$$ MSE(w, b) = \\frac{1}{N} \\sum_{i=1}^N (y_i - (wx_i + b))^2 .$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Minimizing the loss\n",
    "\n",
    "When fitting our neural network model to the fuel efficiency data, we can use gradient descent to find the best values of $w$ and $b$.  Previously, we looked in some detail at how gradient descent works in the single-variable case.  For fitting neural networks, even the simplest case of a single-input neuron requires that we extend our simple formulation of gradient descent to a multi-variable setting.\n",
    "\n",
    "I have decided not to go over the math behind the multi-variable version of gradient descent in class.  This does not suggest in any way that the math is not important!  To the contrary, understanding how gradient descent works in the general case will give you valuable insight into how neural network models are estimated - after all, the multi-variable setting is where the name \"gradient descent\" comes from in the first place!  Furthermore, you pretty much need to have some understanding of the math to make sense of the more advanced optimization algorithms that are often used in deep learning.  So, I definitely encourage you to explore this more on your own.  Also, because we're not covering the math, we're not going to implement the multi-variable algorithm from scratch.  Again, though, I highly encourage you to do this as an optional homework exercise!\n",
    "\n",
    "Okay, so that all said, here is what we _will_ do.  We've already developed some good inuition about gradient descent by exploring the single-variable case.  Today, we'll use experimentation and visualization to try to extend that intuition to the more general case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Investigating training hyperparameters\n",
    "\n",
    "We will begin by exploring two of the most important _hyperparameters_ for training a neural network: the learning rate and the number of parameter update steps to use for training.\n",
    "\n",
    "Let's first get the imports we need and load our trusty old fuel efficiency dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotnine as pn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from skorch import NeuralNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/auto_mpg.csv')\n",
    "#df = pd.read_csv('/blue/zoo4926/share/Jupyter_Content/data/auto_mpg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Re-implement the single-neuron, single-input neural net from the previous notebook, again using $hp$ and $mpg$ as our input and output variables.  Use the previous notebook as a reference.\n",
    "\n",
    "Fit the neural network to the data using $lr = 0.1$ and $epochs = 40$.\n",
    "\n",
    "(Aside: Why am I repeatedly asking you to do these sorts of things rather than just giving you the code?  Because a substantial body of education research suggests that _the more you do and explore yourself_, the _more you are likely to learn_.  Even copying and modifying code from a previous notebook requires some thinking. 🙂)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often very useful to visualize the results of training a neural net.  One fundamental visualization is graphing the training loss.  Let's look at how we can do that.\n",
    "\n",
    "Skorch captures data about the training process in a _class attribute_ called `history`, which we can easily convert to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Graph the change in the loss over the course of your neural net training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore how the learning rate influences the training process by using some experiments to answer a few questions.\n",
    "\n",
    "1. Did your model training converge on the minimum loss?  I.e., did it find the optimal parameter values?\n",
    "2. What happens if we decrease the learning rate?\n",
    "3. What happens if we increase the learning rate?\n",
    "4. What is the general relationship between the learning rate and the number of epochs required for convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The effect of data scaling\n",
    "\n",
    "So far, we've been transforming our data using Scikit-learn's `StandardScaler` before we hand the data off to Skorch and Pytorch.  Does scaling and centering the data influence our results in any way?\n",
    "\n",
    "**Excercise:** Copy your model fitting code from above into the cell below, remove (or comment out) the data scaling/centering, and fit your neural network to the data.  What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, isn't it?  How might you explain your results?  Let's examine visualizations of the loss surface to see if we can gain some insight into what might be happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Conclusions\n",
    "\n",
    "That's it for our investigation of gradient descent.  (Well, kind of.  We'll continue talking about gradient descent in the context of neural net training throughout this unit.)  You should now be able to understand many of the parameters we passed to `NeuralNetRegressor()` when we set up our neural net training.\n",
    "\n",
    "As you've seen (I hope), gradient descent is an extremely powerful, very general method for finding the minimum of a function.  But, it's not always easy to make it \"behave\" in real-world applications, and if used carelessly, the results can be downright misleading.  Treating it as a \"black box\" is generally a bad idea.  Rather, you must have some understanding of how it works to use it well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
