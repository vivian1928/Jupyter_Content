{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to supervised learning\n",
    "\n",
    "## 1. Data\n",
    "\n",
    "First, we need some data!  For this exercise, we'll use a dataset with information about the physical and performance characteristics of automobiles.  Our goal is to predict the fuel efficiency of a car, given its other attributes.  As usual, let's explore the data a bit before we move on to step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/auto_mpg.csv')\n",
    "#df = pd.read_csv('/blue/zoo4926/share/Jupyter_Content/data/auto_mpg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model\n",
    "\n",
    "Next, we need a _model_ that specifies, in a very general sense, how the input data (features) are related to the output data.  For these data, it looks like a linear relationship might be a reasonable model.  Something like this, perhaps:\n",
    "\n",
    "$$ \\hat{mpg} = a + b \\cdot hp. $$\n",
    "\n",
    "Here, $ a $ and $ b $ are unknown constants, $ hp $ is our input, and $ \\hat{mpg} $ is our output.\n",
    "\n",
    "To simplify the notation, let's replace $ hp $ with $ x $ and $ \\hat{mpg} $ with $ \\hat{y} $.\n",
    "\n",
    "$$ \\hat{y} = a + bx $$\n",
    "\n",
    "By now, you might be thinking, \"Hey, this looks like linear regression!\"  If so, you are correct!  Regression is one of the major problem areas in machine learning, and simple linear regression provides an excellent introduction to thinking about problems from a machine learning perspective.  So, we're going to implement linear regression, from first principles, using a machine learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculating the loss\n",
    "\n",
    "Having specified a way to model the relationship between horsepower and fuel efficiency, our main problem now is choosing values for $a$ and $b$, the parameters of our model.  As a first step, consider this question: Given \n",
    "_some_ value of $a$ and $b$, and some observation of $x$ and $y$, which we'll call $x_0$ and $y_0$, how can we evaluate how well our model works?  In other words, how far from \"the truth\" is our model?\n",
    "\n",
    "One option is to calculate the _squared error_:\n",
    "\n",
    "$$ SE = (y_0 - \\hat{y_0})^2 . $$\n",
    "\n",
    "For a variety of reason (some of which we'll see in a moment), the squared error ends up being a good loss function for linear regression.  For a given set of $(x, y)$ observations, $(x_i, y_i); i = 1 ... N$ , we can calculate the average squared error over the entire dataset, which gives us the _mean squared error_:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2 = \\frac{1}{N} \\sum_{i=1}^N (y_i - (a + bx_i))^2 .$$\n",
    "\n",
    "One idea for choosing values for $a$ and $b$ and is to find the values that make $MSE$ as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Minimizing the loss\n",
    "\n",
    "### a. Analytical solution\n",
    "\n",
    "So, how do we find values for $a$ and $b$ that make $MSE$ as small as possible?  Let's explore that question by visualizing the loss function and using pencil and paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we _could_ find an exact, analytical solution to our minimization problem without too much trouble, which is kind of neat.  The solution was:\n",
    "\n",
    "$$ a = \\bar{y} - b \\bar{x} $$\n",
    "\n",
    "$$ b = \\frac{\\sum_{i=1}^N {x_iy_i} - N\\bar{x}\\bar{y}} {\\sum_{i=1}^N {{x_i}^2} - N\\bar{x}^2} $$\n",
    "\n",
    "(I figured I'd better put it here ahead of time in case I messed up the math in class.)\n",
    "\n",
    "**Challenge:** Implement a Python function to calculate values for $a$ and $b$, given vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$.  What estimates does it give for $a$ and $b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do those numbers seem reasonable?  Let's graph our model and see.  We can use the plotnine geometry `geom_abline()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so there obviously is nothing ground-breaking about solving the linear regression problem.  But, I hope this exercise helps build some intuition about what \"line of best fit\" really means.  From a machine learning perspective, it is the line that minimizes our loss function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Numerical solution\n",
    "\n",
    "We were quite fortunate to be able to solve our machine learning problem analytically.  Often, it will be impossible to find an exact solution to the problem of minimizing the loss over a given dataset.  In those cases, we have to use _numerical optimization_ to find an approximate solution.\n",
    "\n",
    "Because we could obtain an exact solution to the linear regression problem, it makes an excellent test case for exploring numerical methods: we know the correct solution, so it is easy to check the results of our numerical solutions.\n",
    "\n",
    "Numerical optimization is a computational cornerstone of machine learning.  My goal here is to introduce numerical optimization methods and illustrate how they work without going into much detail.  We will take a much closer look at the principles behind numerical optimization when we get to artificial neural networks.\n",
    "\n",
    "For now, though, let's implement a numerical solution to the linear regression problem.  We'll begin by implementing our loss function as a Python function.\n",
    "\n",
    "**Challenge:** Implement the $MSE$ loss function for simple linear regression in Python.  Your function should take as arguments values for $a$ and $b$ and the data vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of implementing a numerical optimization algorithm ourselves (don't worry, we'll do that later!), we'll use the sophisticated optimization routines available via the [`minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) function in the `scipy.optimize` package.  The basic interface is\n",
    "\n",
    "`minimize(function, (starting_params), args=(other_args), method='METHOD')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is `minimize()` actually doing?  We can get some idea by visualizing each step of the optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've figured out how to implement simple linear regression using numerical optimization, it doesn't take a big leap to extend our implementation to _multiple linear regression_, where we have more than one predictor variable.\n",
    "\n",
    "Let's look again at the graph of our data with our regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.ggplot(df, pn.aes(x='hp', y='mpg')) + pn.geom_point() + pn.geom_abline(intercept=a, slope=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph suggests that the relationship between horsepower and fuel efficiency is not strictly linear.  Perhaps a quadratic relationship would fit the data better?  Let's try this:\n",
    "\n",
    "$$ \\hat{y} = a + bx + cx^2 . $$\n",
    "\n",
    "Now, we have:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2 = \\frac{1}{N} \\sum_{i=1}^N (y_i - (a + bx_i + cx_i^2))^2 .$$\n",
    "\n",
    "**Challenge:** Building upon the simple linear regression solution we developed above, implement the loss function for this more sophisticated regression problem and use `minimize()` to estimate the parameters of the regression eequation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does our new model fit the data better?  Comparing the final loss function values gives us one way to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the regression line to see how it fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "\n",
    "import numpy as np\n",
    "xvals = np.linspace(df.hp.min(), df.hp.max(), len(df))\n",
    "yvals = params[0] + params[1]*xvals + params[2]*(xvals**2)\n",
    "\n",
    "(pn.ggplot(df, pn.aes(x='hp', y='mpg')) + pn.geom_point() +\n",
    "     pn.geom_line(pn.aes(x=xvals, y=yvals), color='blue', size=1.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you found the correct model parameters, the fit should look pretty good!  We should be suspicious about what our model is saying for cars with more than 200 horsepower, though!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
