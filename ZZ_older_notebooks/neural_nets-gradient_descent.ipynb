{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting neural nets\n",
    "\n",
    "So far, we've investigated the architecture of an artificial neuron and we've used Pytorch and Skorch to fit a very simple neural net to our fuel efficiency data.  Now, we will explore an important question: How do we actually estimate the parameter values of a neural net?  In my opinion, you _must have at least a basic understanding of the answer to this question to be able to effectively use neural nets_.\n",
    "\n",
    "Before we continue, think back to the distant past when we were discussing the basic methods available for fitting supervised learning models.  What methods might we use to estimate the parameter values of a neural net?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a function of a scalar\n",
    "\n",
    "We begin with the simplest minimization problem: finding the minimum of a scalar-valued function of a scalar (i.e., a function of a single, real-valued variable that outputs a single, real value).  To make this concrete, consider the following function:\n",
    "\n",
    "$$ y = (x - 1)^2 + 1 .$$\n",
    "\n",
    "Let's see what this function looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 4, 100)\n",
    "y = (x - 1)**2 + 1\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "pn.ggplot(df, pn.aes(x='x', y='y')) + pn.geom_line() + pn.ylim(0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this function is simple enough that we can find the minimum by simply inspecting the equation and the graph.  Imagine, though, that the function does not have such an obvious minimum, and the only information we have is the function's equation.  Given a random starting $x$ value, how can we design an algorithm to find the function's minimum?  What ideas do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. First attempt: fixed-step minimization\n",
    "\n",
    "Let's try implementing a fixed-step algorithm to minimize our function.  First, write a Python function that implements the (mathematical) function we wish to optimize.\n",
    "\n",
    "As a first step, write a Python function that implements the function we are trying to minimize:\n",
    "\n",
    "$$ y = (x - 1)^2 + 1 .$$\n",
    "\n",
    "Name your function `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's implement and test our minimization function. Here is a sketch of the algorithm:\n",
    "\n",
    "1. Start at some value of $x$ (could be random).\n",
    "2. Change $x$ by a fixed step size in the direction that makes $y$ smaller.\n",
    "3. Repeat 2 for the desired number of steps.\n",
    "\n",
    "Try your hand at implementing this algorithm. To help you get started, I've provided the function specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_1(f, start_x, stepsize, num_steps):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your function implemented, try running it to find the value of $x$ that minimizes our equation.  Try different starting values and step sizes.  How well does it work?  Most important, does it find the minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Discuss and revise\n",
    "\n",
    "Well, that kind of worked, but had some shortcomings.  Most important:\n",
    "\n",
    "1. It didn't actually converge on the minimum.\n",
    "2. If we start far from the minimum, it will take a _long_ time to get there.\n",
    "3. It doesn't scale very well to multi-dimensional parameter spaces.\n",
    "\n",
    "How can we improve this?  Again, what ideas do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Second attempt: using more information about the function\n",
    "\n",
    "For our next attempt, write a Python function that implements the _derivative_ of the function we are trying to minimize:\n",
    "\n",
    "$$ y = (x - 1)^2 + 1 ,$$\n",
    "$$ \\frac{dy}{dx} = 2x - 2 .$$\n",
    "\n",
    "Name your function `d_f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's implement and test our new minimization function in Python. Here is a sketch of the algorithm:\n",
    "\n",
    "1. Define a constant value, $\\gamma$, called the _learning rate_.\n",
    "2. Start at some value of $x$ (could be random).\n",
    "3. Compute the derivative of $y$ with respect to $x$ at the current value of $x$.\n",
    "4. Change $x$ by a step size _proportional to the derivative_ in the direction that makes $y$ smaller; i.e., $$x_{t+1} = x_t - \\gamma \\frac{dy}{dx}(x_t)$$\n",
    "5. Repeat 3 and 4 for the desired number of steps.\n",
    "\n",
    "Try your hand at implementing this algorithm. To help you get started, I've provided the function specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_2(f, df, start_x, learning_rate, num_steps):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your function implemented, try running it to find the value of ùë• that minimizes our equation. Try different starting values and learning rates. How well does it work? Most important, does it find the minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just implemented is the simplest case of a very general optimization algorithm called _gradient descent_.  Gradient descent is the key to fitting neural nets.  Next, we'll look at how gradient descent works in the more complicated -- and more realistic -- setting where we are minimizing a loss function that accepts a vector of parameter values instead of a single $x$ value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
