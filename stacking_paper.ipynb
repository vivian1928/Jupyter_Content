{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b604139-851e-48da-9697-19f41142a9c6",
   "metadata": {},
   "source": [
    "# Papers With[out] code\n",
    "\n",
    "This is the notebook from our attempt at replicating the Kumar *et al.* 2022 paper [Optimized Stacking Ensemble Learning Model for Breast Cancer Detection and Classification Using Machine Learning](https://www.mdpi.com/2071-1050/14/21/13998) in class.\n",
    "\n",
    "\n",
    "This paper uses the breast_cancer data we've looked at before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f657d6-1bc3-4023-a482-63ee03c8b37d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the data to remind ourselves what we have\n",
    "breast_cancer = load_breast_cancer(as_frame=True) # We need to load the data from sklearn rather than sns\n",
    "breast_cancer.frame.head()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bd8a83b-f941-42f7-92f6-f6aded088c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in the data as nice, X, y datasets\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c02333-296c-49f0-8318-8650c27d279e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into training/testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae11cdb-53a2-47ac-9f80-e4714b90f67a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "# Run a Knearest neighbors model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the KNN classifier\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a8583e-3190-4a24-a954-6e00b8c579bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on training data: 0.9577464788732394\n",
      "Model score on testing data: 0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "# Run logistic regression model\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    ('logistic', LogisticRegression(solver = 'liblinear'))])\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X, y)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'Model score on training data: {model.score(X_train,y_train)}')\n",
    "print(f'Model score on testing data: {model.score(X_test,y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132c03e3-6876-4814-9f55-38c6e99674d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 0 ns, total: 1.58 s\n",
      "Wall time: 1.59 s\n",
      "{'svc__C': 1.25, 'svc__gamma': 0.004}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91        55\n",
      "           1       0.98      0.91      0.94        88\n",
      "\n",
      "    accuracy                           0.93       143\n",
      "   macro avg       0.92      0.94      0.93       143\n",
      "weighted avg       0.93      0.93      0.93       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SVM model\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "   ('svc', SVC(kernel='rbf', class_weight='balanced'))\n",
    "  ])\n",
    "param_grid = {'svc__C': [0.1,0.5,0.75,1,1.25],\n",
    "              'svc__gamma': [ 0.003 , 0.004, 0.005,0.006]}\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "\n",
    "model = grid.best_estimator_\n",
    "y_fit = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f161f603-b225-4305-8656-acb77bdd9207",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "# Run Random Forest model\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import metrics\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Train Accuracy: {accuracy_score(model.predict(X_train),y_train)}')\n",
    "print(f'Test Accuracy: {accuracy_score(model.predict(X_test),y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985e1e1-c6b1-4882-9b94-e13adea91815",
   "metadata": {},
   "source": [
    "## Individual models\n",
    "\n",
    "Each of our individual models did reasonably well with the set of hyperparameters used. We didn't have much time to test other settings, but in most cases, are within a few % points of what the paper reported for thier individual models.\n",
    "\n",
    "## Stack the models\n",
    "\n",
    "This is where we didn't have much time to work, and also didn't get into the detail of how the paper implemented the stacking. We went with the logistic regression method we used earlier.\n",
    "\n",
    "The results are a bit underwhelming...worse than many individual models. So, this needs some more work, but should be a decent start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1269c252-d636-42bb-baa4-4469fc30fe86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Stacking classifier:  0.933 (0.065)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('lr', LogisticRegression(solver = 'liblinear')))\n",
    "    level0.append(('knn', KNeighborsClassifier(n_neighbors=5)))\n",
    "    level0.append(('svm', SVC(kernel='rbf', class_weight='balanced', C=1.25, gamma=0.005)))\n",
    "    level0.append(('randfor',  RandomForestClassifier(n_estimators=100, random_state=0)))\n",
    "    # define meta learner model\n",
    "    level1 = LogisticRegression(max_iter=100000)\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "scores = evaluate_model(model, X_test, y_test)\n",
    "print('>%s %.3f (%.3f)' % (\"Stacking classifier: \", mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a752d-cff0-4114-920c-10a55a20dca7",
   "metadata": {},
   "source": [
    "## Take home\n",
    "\n",
    "1. You can read papers and implement their methods\n",
    "1. You can make ML models that are similar to what is publishable results from only a few years ago, and do it in class with 10 minutes to work!\n",
    "1. Hyperparameters mater! We would need to play with these to do better.\n",
    "1. Publish your code!! People can't replicate your work without your code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UFRC Python-3.10",
   "language": "python",
   "name": "python3-3.10-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
