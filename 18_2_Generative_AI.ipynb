{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generative AI \n",
    "Generative AI refers to a subset of artificial intelligence models and algorithms designed to generate new data samples  that closely mirror the statistical properties of a target dataset. Unlike traditional classification and regression models, which predict labels or values based on input features, generative models aim to understand and replicate the complex distributions of input data, allowing them to produce entirely new data points that are similar or even indistinguishable to those in the training set.\n",
    "\n",
    "## Main Classes of Generative AI Models\n",
    "\n",
    "### 1. **Generative Adversarial Networks (GANs):**\n",
    "- **Overview:** Consist of two neural networks, the generator and the discriminator, that are trained simultaneously in an adversarial process.\n",
    "- **Applications:** Widely used for tasks like image generation, super-resolution, style transfer, and data augmentation.\n",
    "\n",
    "### 2. **Variational Autoencoders (VAEs):**\n",
    "- **Overview:** Based on Bayesian inference principles, VAEs encode input data into a distribution in latent space and decode from this space to reconstruct the input.\n",
    "- **Applications:** Employed in image generation, anomaly detection, and semi-supervised learning tasks.\n",
    "\n",
    "### 3. **Autoregressive Models:**\n",
    "- **Overview:** These models generate sequences by predicting each new piece of data conditioned on the previously generated pieces.\n",
    "- **Applications:** Utilized for text generation, image synthesis, and time series forecasting.\n",
    "\n",
    "### 4. **Diffusion Models:**\n",
    "- **Overview:** Generate data by gradually denoising a sample from a simple distribution over a series of steps, guided by a trained neural network.\n",
    "- **Applications:** Shown impressive results in high-fidelity image generation, audio synthesis, and molecular design.\n",
    "\n",
    "Each class has its strengths and is chosen based on specific task requirements, such as sample quality, training stability, and computational efficiency. The field continues to evolve rapidly, introducing new models and improving existing ones.\n",
    "\n",
    "## Differences from discriminative models\n",
    "In this course, at least so far, we have been focusing on class of models referred to as `discriminative models`. Deep learning classifiers and regressors are two examples of such models. Both classification and regression models focus on mapping input data to known outputs or labels, making them excellent for predictive tasks but not suited for generating new data.\n",
    "\n",
    "## Potential in Biology\n",
    "Generative models have significant potential in biology, offering innovative ways to tackle complex problems:\n",
    "\n",
    "* __Drug Discovery__: Generative models can propose novel molecular structures with desired properties, accelerating the identification of potential new drugs. See [examples](https://blogs.nvidia.com/blog/generative-ai-proteins-evozyne/)\n",
    "* __Synthetic Biology__: These models can design new genetic sequences or synthetic organisms with specific functions, supporting advances in bioengineering.\n",
    "* __Data Augmentation__: Generative AI can create additional training samples for rare conditions or species, enhancing the performance of classification models in biology.\n",
    "* __Understanding Complex Systems__: By generating data under different simulated conditions, generative models help in understanding complex biological systems and interactions.\n",
    "\n",
    "Today, we will concentrate in more mundane tasks, such as increasing the resolution of bioimages, or generating missing parts of images. To do that, we will use a class of generative models called '_Generative Adversarial Networks_'.\n",
    "\n",
    "## Generative Adversarial Networks (GANs)\n",
    "GANs are a powerful class of generative models consisting of two neural networks, the generator and the discriminator, which are trained simultaneously in a competitive setting:\n",
    "\n",
    "* Generator: Learns to produce data that mimics the training dataset. In biology, this could involve generating synthetic genomic sequences or realistic cell images.\n",
    "* Discriminator: Learns to distinguish between real data from the training set and fake data produced by the generator. Its feedback helps improve the generator.\n",
    "The adversarial process leads to the generator creating highly realistic data, making GANs particularly effective for tasks requiring high-quality synthetic data generation. Please see the diagram below that was produced by [Google](https://developers.google.com/machine-learning/gan/gan_structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GAN](images/gan_diagram.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating random digits\n",
    "\n",
    "Today, our first task will be to create a simple GAN implementation and train it to generate digit images that resemble those from the MNIST dataset, a collection of handwritten digits widely used in machine learning. The code we'll explore consists of two main components: the Generator, which will learn to produce images akin to MNIST digits, and the Discriminator, whose job is to distinguish between real MNIST images and the fakes produced by our Generator. As we dive into the code, keep in mind that the Generator and Discriminator are essentially in a continuous game of cat and mouse, improving through each iteration of our training loop. This dynamic interaction is what makes GANs both challenging and incredibly fascinating. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's load the MNIST dataset from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize and reshape the images\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "# Batch and shuffle the data\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a simple generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*128, use_bias=False, input_shape=(200,)))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((7, 7, 128)))  # Reshape to a 7x7x128 tensor\n",
    "\n",
    "    # Upsample to 28x28x1\n",
    "    model.add(layers.Conv2DTranspose(1, (4, 4), strides=(4, 4), padding='same', use_bias=False, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the generator by defining a discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our losses (i.e., error metric). We will use binary cross entropy because the task at hand is quite simple. Either the model will consider a sample \"real\" or \"fake\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(5e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create some code to display images during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_display_images(model, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we can define our approach for training the model and storing the losses/data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, 200])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "def train(dataset, epochs, seed):\n",
    "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "        epoch_gen_loss = []\n",
    "        epoch_disc_loss = []\n",
    "        \n",
    "        for image_batch in dataset:\n",
    "            gen_loss, disc_loss = train_step(image_batch)\n",
    "            epoch_gen_loss.append(gen_loss.numpy())\n",
    "            epoch_disc_loss.append(disc_loss.numpy())\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        generator_losses.append(np.mean(epoch_gen_loss))\n",
    "        discriminator_losses.append(np.mean(epoch_disc_loss))\n",
    "\n",
    "        # Display images and print loss every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'\\nEpoch {epoch + 1} completed')\n",
    "            #print(f'Generator loss: {generator_losses[-1]}, Discriminator loss: {discriminator_losses[-1]}')\n",
    "            generate_and_display_images(generator, seed)\n",
    "\n",
    "    # Final display\n",
    "    if epochs % 10 != 0:\n",
    "        generate_and_display_images(generator, seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = tf.random.normal([16, 200])  # 'num_examples_to_generate' is typically 16 for a 4x4 grid\n",
    "EPOCHS = 100  # Set the number of epochs\n",
    "train(train_dataset, EPOCHS, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Applications: Super resolution!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook has been adapted from an official [tensorflow example](https://github.com/tensorflow/examples/blob/1fc8f6af1d542ed358d12dec88156d4cbf70d9d4/lite/examples/super_resolution/ml/super_resolution.ipynb)\n",
    "\n",
    "In particular, we will use a GAN for the task of recovering a high resolution (HR) image from its low resolution counterpart.\n",
    "\n",
    "The model we are going to use here is ESRGAN\n",
    "([ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/1809.00219)). \n",
    "\n",
    "The model in our shared drive has been converted from this\n",
    "[implementation](https://tfhub.dev/captain-pool/esrgan-tf2/1) hosted on the tensorflow Hub. This model upsamples a 50x50 low resolution image to a 200x200 high resolution image (scale factor=4). If you want a different input size or scale factor, you need to re-convert or re-train the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the model path and load a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "esrgan_model_path = '/blue/bsc4892/share/super-resolution/ESRGAN.tflite'\n",
    "test_img_path = tf.keras.utils.get_file('lr.jpg', \n",
    "                                        'https://raw.githubusercontent.com/tensorflow/examples/master/lite/examples/super_resolution/android/app/src/main/assets/lr-1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple image of a moth, at very low resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the image\n",
    "img = Image.open(test_img_path)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axes ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to upsample this image using interpolation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = Image.open(test_img_path)\n",
    "img = img.convert('RGB')\n",
    "lr = np.array(img)\n",
    "lr = tf.expand_dims(lr, axis=0)\n",
    "lr = tf.cast(lr, tf.float32)\n",
    "\n",
    "lr = tf.cast(tf.squeeze(lr, axis=0), tf.uint8)\n",
    "bicubic = tf.image.resize(lr, [200, 200], tf.image.ResizeMethod.BICUBIC)\n",
    "bicubic = tf.cast(bicubic, tf.uint8)\n",
    "plt.subplot(1, 2, 2)   \n",
    "plt.title('Bicubic')\n",
    "plt.imshow(bicubic.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look that great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the model and see if we obtain a better image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = np.array(img)\n",
    "lr = tf.expand_dims(lr, axis=0)\n",
    "lr = tf.cast(lr, tf.float32)\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=esrgan_model_path)\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "#Resize the input tensor.\n",
    "interpreter.resize_tensor_input(input_details[0]['index'],[1,50,50,3])\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Run the model\n",
    "interpreter.set_tensor(input_details[0]['index'], lr)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Extract the output and postprocess it\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "sr = tf.squeeze(output_data, axis=0)\n",
    "sr = tf.clip_by_value(sr, 0, 255)\n",
    "sr = tf.round(sr)\n",
    "sr = tf.cast(sr, tf.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generating a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = tf.cast(tf.squeeze(lr, axis=0), tf.uint8)\n",
    "plt.figure(figsize = (1, 1))\n",
    "plt.title('LR')\n",
    "plt.imshow(lr.numpy());\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)        \n",
    "plt.title(f'ESRGAN (x4)')\n",
    "plt.imshow(sr.numpy());\n",
    "\n",
    "bicubic = tf.image.resize(lr, [200, 200], tf.image.ResizeMethod.BICUBIC)\n",
    "bicubic = tf.cast(bicubic, tf.uint8)\n",
    "plt.subplot(1, 2, 2)   \n",
    "plt.title('Bicubic')\n",
    "plt.imshow(bicubic.numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications : Synthesis\n",
    "\n",
    "In this next section, we will use a GAN for the task of image extension/synthesis.\n",
    "\n",
    "The model we are going to use here is [Boundless](https://arxiv.org/pdf/1908.07007.pdf) (Boundless: Generative Adversarial Networks for Image Extension).\n",
    "\n",
    "The model in our shared drive has been downloaded from [here](https://www.tensorflow.org/hub/tutorials/boundless). This model requires input images at 257x257 resolution. If you want a different input size or scale factor, you need to re-convert or re-train the original model. In this exercise, we will use the same image that we used above for this task.\n",
    "\n",
    "\n",
    "### Core Aspects:\n",
    "\n",
    "- **Functionality**: Primarily focused on image extension, the Boundless model predicts the continuation of scenes, textures, and patterns from the available visual information in the input image.\n",
    "- **Architecture**: The model leverages a convolutional neural network (CNN) structure, trained on a diverse dataset to understand and replicate a wide range of image features and styles.\n",
    "- **Applications**: While its uses span artistic and practical realms, such as digital art creation and content generation, the model's application is grounded in its ability to reliably predict image extensions based on learned patterns.\n",
    "\n",
    "Let's start by loading the model and running the image through it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boundless_model_path = '/blue/bsc4892/share/super-resolution/Boundless.tflite'\n",
    "\n",
    "img = Image.open(test_img_path)\n",
    "img = img.convert('RGB')\n",
    "lr = np.array(img)\n",
    "lr = tf.image.resize(lr, [257, 257])\n",
    "lr = tf.expand_dims(lr, axis=0)\n",
    "lr = tf.cast(lr, tf.float32)\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "model = tf.lite.Interpreter(model_path=boundless_model_path)\n",
    "\n",
    "model.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = model.get_input_details()\n",
    "output_details = model.get_output_details()\n",
    "\n",
    "# Set the resized input tensor\n",
    "model.set_tensor(input_details[0]['index'], lr/255)\n",
    "\n",
    "# Invoke the model\n",
    "model.invoke()\n",
    "\n",
    "# To retrieve the output, you need to use the output tensor indices from 'output_details'\n",
    "# Here's an example of how you might retrieve the outputs:\n",
    "generated_image_index = output_details[0]['index']\n",
    "masked_image_index = output_details[1]['index']\n",
    "\n",
    "generated_image = model.get_tensor(generated_image_index)\n",
    "masked_image = model.get_tensor(masked_image_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure 'lr', 'generated_image', and 'masked_image' are in the correct format for visualization\n",
    "# If 'lr' was expanded with a batch dimension and cast to float, convert it back for visualization\n",
    "lr_vis = tf.squeeze(lr, axis=0)  # Remove batch dimension if present\n",
    "lr_vis = tf.cast(lr_vis, tf.uint8)  # Convert back to uint8\n",
    "\n",
    "# Convert 'generated_image' and 'masked_image' back to uint8 if they are not already\n",
    "generated_image_vis = tf.cast(tf.squeeze(generated_image, axis=0)*255, tf.uint8)  # Assuming 'generated_image' might have a batch dimension\n",
    "masked_image_vis = tf.cast(tf.squeeze(masked_image, axis=0)*255, tf.uint8)  # Assuming 'masked_image' might have a batch dimension\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Display the original LR image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(lr_vis.numpy())\n",
    "plt.title('Original LR Image')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "# Display the masked image\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(masked_image_vis.numpy())\n",
    "plt.title('Masked Image')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "# Display the generated (super-resolved) image\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(generated_image_vis.numpy())\n",
    "plt.title('Generated Image')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.15",
   "language": "python",
   "name": "tensorflow-2.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
