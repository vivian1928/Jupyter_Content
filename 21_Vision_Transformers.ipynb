{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf19716-5c77-4258-8d92-da82d4c29dd4",
   "metadata": {},
   "source": [
    "# Vision Transformers\n",
    "\n",
    "Not too long after Vaswani *et al.*'s [*Attention is all you need*](https://arxiv.org/abs/1706.03762) paper started revolutionizing NLP models, people started to wonder if transformers could be applied to other domains.\n",
    "\n",
    "In 2021 the Alexey Dosovitskiy *et al.* (from Google Brain) published a preprint called [*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*](https://arxiv.org/abs/2010.11929) where they applied transformers to computer vision. Thier intro starts:\n",
    "\n",
    "> Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset (Devlin et al., 2019). Thanks to Transformersâ€™ computational efficiency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the models and datasets growing, there is still no sign of saturating performance.\n",
    "\n",
    "After noting that some attempts to apply transformers to computer vision had not been terribly successful, largely because they continued to use CNN-like approaches, Dosovitskiy *et al.* state:\n",
    "\n",
    "> Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.\n",
    "\n",
    "And while they find that with smaller datasets, results are less than stellar, Dosovitskiy *et al.* find that their Vision Transformer (ViT):\n",
    "\n",
    "> When pre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. In particular, the best model reaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100, and 77.63% on the VTAB suite of 19 tasks.\n",
    "\n",
    "Here's figure 1 from Dosovitskiy *et al.* (2021):\n",
    "\n",
    "![Figure 1 from Dosovitskiy *et al.* 2021 ViT architecture](images/ViT_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc988390-736f-4198-a647-23f85571ea7f",
   "metadata": {},
   "source": [
    "## First a word about Python modules and Jupyter kernels...\n",
    "\n",
    "Before getting into using transformers, we need to learn a bit more about Python modules and Jupyter kernels. Check out [this page on the class website](https://aibiology.github.io/vit_mamba_setup.html).\n",
    "\n",
    "## And now, two options...\n",
    "\n",
    "At this point, in looking for examples to work through, I ran into some challenges. First, this is all relatively new, so there are not a lot of great examples out there. There are several that walk through *training* a ViT-like model, but not many that go into *using* ViT with transfer learning, fine tuning, and application to data.\n",
    "\n",
    "I did find two that seem good. \n",
    "\n",
    "### Option 1\n",
    "\n",
    "The first is in this notebook and is from  Philipp Schmid's blog post [*Image Classification with Hugging Face Transformers and `Keras`*](https://www.philschmid.de/image-classification-huggingface-transformers-keras). The problem is, and I might have tried subsampling the data if I had more time, is that it takes about 1-hour per epoch to train as it is.\n",
    "\n",
    "### Option 2\n",
    "\n",
    "The second example, which I think we'll use in class is in notebook [21b_vit-fine-tuning.ipynb](21b_vit-fine-tuning.ipynb) and is from [Ruaf Momin's Kaggle notebook](https://www.kaggle.com/code/raufmomin/vision-transformer-vit-fine-tuning).\n",
    "\n",
    "### General observation\n",
    "\n",
    "And a general observation about both options...While both options use fine tuning of a ViT mobel, neither freezes the weights of the main ViT model as we did in transfer learning. Honestly don't know if that's an oversight, strategy, or what.\n",
    "\n",
    "\n",
    "## Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4d4a4-abab-4305-aaa4-6e39fae60e1a",
   "metadata": {},
   "source": [
    "Much of this example comes from Philipp Schmid's blog post [*Image Classification with Hugging Face Transformers and `Keras`*](https://www.philschmid.de/image-classification-huggingface-transformers-keras).\n",
    "\n",
    "This example uses the EuroSAT satellite images from [here](https://github.com/phelber/eurosat) published in:\n",
    "* Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. [https://doi.org/10.48550/arXiv.1709.00029](https://doi.org/10.48550/arXiv.1709.00029)\n",
    "\n",
    "*  Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. Patrick Helber, Benjamin Bischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote Sensing Symposium, 2018. [https://doi.org/10.1109/IGARSS.2018.8519248](https://doi.org/10.1109/IGARSS.2018.8519248)\n",
    "\n",
    "For the class, the dataset has been installed at: `/blue/bsc48926/share/EuroSAT_RGB/`\n",
    "\n",
    "Here's the description of the data and a composite image of patches:\n",
    "\n",
    "> In this study, we address the challenge of land use and land cover classification using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible provided in the Earth observation program Copernicus. We present a novel dataset based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images. \n",
    "\n",
    "![EuroSAT image](https://github.com/phelber/EuroSAT/raw/master/eurosat_overview_small.jpg?raw=true)\n",
    "\n",
    "The land use classes are: `AnnualCrop`,  `Forest`,  `HerbaceousVegetation`,  `Highway`,  `Industrial`,  `Pasture`,  `PermanentCrop`,  `Residential`,  `River`,  `SeaLake`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4aaa1-bae0-447e-bea6-74c1c1baa3d3",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "\n",
    "I have opted to stick with the image loading and processing pipeline used by Phillip rather than try to convert this to use the `image_dataset_from_directory`, which I *think* should work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa75f37-4dce-4a1b-8185-da3517ef5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import datasets\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import ViTFeatureExtractor\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import TFViTForImageClassification, create_optimizer\n",
    "\n",
    "\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard as TensorboardCallback, EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3961c-faf1-4335-acd0-5d7fd9ac58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model ID and path to images\n",
    "model_id = \"google/vit-base-patch16-224-in21k\"\n",
    "eurosat_path=\"/blue/bsc4892/share/EuroSAT_RGB/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6fb57-8482-42d6-b67c-8e9622de9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_folder_dataset(path):\n",
    "  \"\"\"creates `Dataset` from image folder structure\"\"\"\n",
    "  \n",
    "  # get class names by folders names\n",
    "  _CLASS_NAMES=os.listdir(path)\n",
    "  # defines `datasets` features`\n",
    "  features=datasets.Features({\n",
    "                      \"img\": datasets.Image(),\n",
    "                      \"label\": datasets.features.ClassLabel(names=_CLASS_NAMES),\n",
    "                  })\n",
    "  # temp list holding datapoints for creation\n",
    "  img_data_files=[]\n",
    "  label_data_files=[]\n",
    "  # load images into list for creation\n",
    "  for img_class in os.listdir(path):\n",
    "    for img in os.listdir(os.path.join(path,img_class)):\n",
    "      path_=os.path.join(path,img_class,img)\n",
    "      img_data_files.append(path_)\n",
    "      label_data_files.append(img_class)\n",
    "  # create dataset\n",
    "  ds = datasets.Dataset.from_dict({\"img\":img_data_files,\"label\":label_data_files},features=features)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe629b70-b6f7-4707-aff0-51628713f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "eurosat_ds = create_image_folder_dataset(eurosat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286744d-3fc3-4c9e-8f35-edac340de164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the class labels\n",
    "img_class_labels = eurosat_ds.features[\"label\"].names\n",
    "print(img_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcfeb5d-9597-4daf-b2ee-7ee678eb98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "# Note this step takes several minutes to run--not sure why...\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07edaeee-add0-44ac-90bb-7b632801fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn more about data augmentation here: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "\n",
    "\n",
    "# data_augmentation = keras.Sequential(\n",
    "#     [\n",
    "#         layers.Resizing(feature_extractor.size, feature_extractor.size),\n",
    "#         layers.Rescaling(1./255),\n",
    "#         layers.RandomFlip(\"horizontal\"),\n",
    "#         layers.RandomRotation(factor=0.02),\n",
    "#         layers.RandomZoom(\n",
    "#             height_factor=0.2, width_factor=0.2\n",
    "#         ),\n",
    "#     ],\n",
    "#     name=\"data_augmentation\",\n",
    "# )\n",
    "\n",
    "# # use keras image data augementation processing\n",
    "# def augmentation(examples):\n",
    "#     # print(examples[\"img\"])\n",
    "#     examples[\"pixel_values\"] = [data_augmentation(image) for image in examples[\"img\"]]\n",
    "#     return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f973c73-f50e-4f93-b0e6-ed80abf585f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "\n",
    "# basic processing (only resizing)\n",
    "def process(examples):\n",
    "    examples.update(feature_extractor(examples['img'], ))\n",
    "    return examples\n",
    "\n",
    "# we are also renaming our label col to labels to use `.to_tf_dataset` later\n",
    "eurosat_ds = eurosat_ds.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb29631-de17-4c73-91f4-30521058235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset\n",
    "processed_dataset = eurosat_ds.map(process, batched=True)\n",
    "processed_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e4c0a-e3e0-426c-9f4e-3791fd5669d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test datasets\n",
    "# test size will be 15% of train dataset\n",
    "test_size=.15\n",
    "\n",
    "processed_dataset = processed_dataset.shuffle().train_test_split(test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9285f-3974-446d-95f9-5c7f37b09418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id2label = {str(i): label for i, label in enumerate(img_class_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_train_epochs = 5\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate=0.01\n",
    "num_warmup_steps=0\n",
    "output_dir=model_id.split(\"/\")[1]\n",
    "#hub_token = HfFolder.get_token() # or your token directly \"hf_xxx\"\n",
    "hub_model_id = f'{model_id.split(\"/\")[1]}-euroSat'\n",
    "fp16=True\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# Comment this line out if you're using a GPU that will not benefit from this\n",
    "if fp16:\n",
    "  tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b648e1-a3de-4aea-991c-d4f1f4f3a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = processed_dataset[\"train\"].to_tf_dataset(\n",
    "   columns=['pixel_values'],\n",
    "   label_cols=[\"labels\"],\n",
    "   shuffle=True,\n",
    "   batch_size=train_batch_size,\n",
    "   collate_fn=data_collator)\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = processed_dataset[\"test\"].to_tf_dataset(\n",
    "   columns=['pixel_values'],\n",
    "   label_cols=[\"labels\"],\n",
    "   shuffle=True,\n",
    "   batch_size=eval_batch_size,\n",
    "   collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628630c-be0a-4f0f-ac0f-525562bea936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizer wight weigh decay\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afae4d8-c8d9-4b56-bd6d-fe6b91ff139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained ViT model\n",
    "model = TFViTForImageClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=len(img_class_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# define loss\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# define metrics \n",
    "metrics=[\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(3, name=\"top-3-accuracy\"),\n",
    "]\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=metrics\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13900e-ad6a-4be9-8451-13a855553855",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[]\n",
    "\n",
    "callbacks.append(TensorboardCallback(log_dir=os.path.join(output_dir,\"logs\")))\n",
    "callbacks.append(EarlyStopping(monitor=\"val_accuracy\",patience=1))\n",
    "\n",
    "#We won't be using HuggingFace hub\n",
    "#if hub_token:\n",
    "#  callbacks.append(PushToHubCallback(output_dir=output_dir,\n",
    "#                                     hub_model_id=hub_model_id,\n",
    "#                                     hub_token=hub_token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d0f58-1b91-4286-961d-2b4e9ef3d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    callbacks=callbacks,\n",
    "    epochs=num_train_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4be4b-6955-453b-a72b-40ec38fe79fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViT",
   "language": "python",
   "name": "vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
