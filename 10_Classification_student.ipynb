{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reminders about Supervised, Unsupervised and Reinforcement Learning\n",
    "\n",
    "One of the ways of differentiating categories of machine learning algorithms is to split them into **supervised**, **unsupervised** and **reinforcement** learning methods. \n",
    "\n",
    "With **supervised** methods, the training data come with the outcome you are trying to predict--they are labeled. For example, with linear regression, we have linked input and output values and are predicting the function that relates them. With image classification, we might start with a set of images that are labeled as cats, dogs, people, etc., and then train the algorithm to learn to classify a new image into these classes.\n",
    "\n",
    "With **unsupervised** methods, we have unlabeled data (no y-values or pre-defined categories) and ask the algorithm to learn patterns from the data itself. These methods usually try to categorize the observations.\n",
    "\n",
    "With **reinforcement learning**, the training data is also unlabeled, but the system received feedback for its actions/categorizations. Game systems are a classic example of this--learn to play chess by playing games and learning what moves lead to a winning strategy. Robotics and autonomous vehicles are other applications--learn how to climb stairs by getting points for actions that leads to going up stairs.\n",
    "\n",
    "There are also methods that mix these.\n",
    "\n",
    "## Types of Supervised Learning\n",
    "\n",
    "The two main types of supervised learning are **classification** and **regression**.\n",
    "\n",
    "In **classification**, the response variable (y) is categorical or discrete. For example, this image is a cat, dog, bird, etc.; you do or do not have some disease; this plant is a desired crop or a weed to kill; etc.\n",
    "\n",
    "In **regression**, the response variable (y) is continuous or numerical. in the automobile fuel efficiency data we've been working with, miles per gallon is the response variable and the regression approaches we've been looking at are finding the model that takes the input (hp) and predicts the output (mpg).\n",
    "\n",
    "The image below from the [maplearn.ml package documentation](https://maplearn.readthedocs.io/en/latest/maplearn.ml.html) provides a nice visual of the general idea of classification and regression.\n",
    "\n",
    "![Classification vs regression. Image from maplearn.ml documentation](images/classif_reg.maplearn.png)\n",
    "\n",
    "\n",
    "## Types of classification\n",
    "\n",
    "Many methods have been developed for classification and the list below is certainly not comprehensive. Nor can we hope to cover all of them in this class. But hopefully this list gives you a sense of the diversity and places to start looking.\n",
    "\n",
    "* Linear Classifiers\n",
    "  * Logistic regression\n",
    "  * Naive Bayes classifier\n",
    "  * Fisher’s linear discriminant\n",
    "* Kernel methods\n",
    "  * Support vector machines\n",
    "  * Least squares support vector machines\n",
    "* k-nearest neighbors\n",
    "* Decision trees\n",
    "  * Random forests\n",
    "* Neural networks\n",
    "* Learning vector quantization\n",
    "\n",
    "\n",
    "We will look at a few of these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Despite its name, **logistic regression** is a **classification** algorithm. Logistic regression estimates the probability of an observation belonging to each class, given the values of the predictor variables (features). This is then used, in conjunction with a threshold to predict discrete (most often binary) class association--1/0, yes/no, true/false--based on input data. Logistic regression predicts the probability of occurrence of an event by fitting a **logit function**. \n",
    "\n",
    "A common example of logistic regression is the task of classifying email as spam or not-spam. I found this [Introduction to Logistic Regression](https://courses.lumenlearning.com/introstats1/chapter/introduction-to-logistic-regression/) particularly helpful in preparing this.\n",
    "\n",
    "Logistic regression is a generalized linear model where the outcome is a binary (two-levels) categorical variable. What we are looking for is actually **probabilities**: What is the probability of each possible outcome? Outcome 1 (spam) happens with the probability of $p_i$ and outcome 0 (not-spam) with the probability $1-p_i$.\n",
    "\n",
    "The **odds** of an event are the ratio of the probability that the event occurs to the probability that it doesn't. \n",
    "\n",
    "$$ odds\\,of\\,event = \\frac{p}{1-p} $$\n",
    "\n",
    "Remember our standard linear regression equation:\n",
    "\n",
    "$$ \\hat{y} = \\beta_0  + \\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i$$\n",
    "\n",
    "In this, $\\hat{y}$ can have any range. To get a probability, we need to transform $\\hat{y}$ such that it has values between 0 and 1. A common transformation is the **logit transformation**.\n",
    "\n",
    "$$ logit(y) = ln(odds) = ln(\\frac{p_i}{1-p_i}) $$\n",
    "\n",
    "So we end up with:\n",
    "\n",
    "$$  ln(\\frac{p_i}{1-p_i}) = \\beta_0  + \\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i$$  \n",
    "\n",
    "To solve for $p$, we can take the antilog and do a bunch of algebra to end up with:\n",
    "\n",
    "$$ p = \\frac{1}{1+e^{-(\\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i)}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression and the Sigmoid and Natural Log Functions\n",
    "\n",
    "Logistic regression makes use of two key functions that we'll review here.\n",
    "\n",
    "First, the **sigmoid function** pictured below. The sigmoid function has values very close to (but not equal to) 0 or 1 for most of its distribution, making it helpful for classification.\n",
    "\n",
    "$$ \n",
    "    \\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{1}{1+exp^{-x}}\n",
    "$$\n",
    "And think of $\\sigma(x)$ as *y*.\n",
    "\n",
    "#### The Sigmoid function $\\frac{1}{1+exp^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate sigmoid graph below\n",
    "import plotnine as pn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "x = np.linspace(-10,10, num=100)\n",
    "y = 1/(1+np.exp(-x))\n",
    "\n",
    "df = pd.DataFrame(x,y)\n",
    "\n",
    "pn.ggplot(df, pn.aes(x='x', y='y')) + pn.geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Natural log function: $log(x)$  in green, $log(1-x)$ in blue\n",
    "\n",
    "The natural log function is shown below. Note that in Python, `math.log(x)` and `numpy.log(x)` represent the natural logarithm of `x`, so we'll use **log** vs **ln**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.001,0.999, num=100)\n",
    "y = np.log(x)\n",
    "\n",
    "\n",
    "label=\"log(x)\"\n",
    "df = pd.DataFrame(x,y)\n",
    "\n",
    "pn.ggplot(df, pn.aes(x='x', y='y')) + pn.geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Mirko Stojiljković's Logistic Regression in Python](https://realpython.com/logistic-regression-python/)\n",
    "\n",
    "> Your goal is to find the **logistic regression function $p(x)$** such that the **predicted responses $p(x_i)$** are as close as possible to the **actual response $y_i$** for each observation $i=1,\\ldots,n$. Remember that the actual response can be only 0 or 1 in binary classification problems! This means that each $p(x_i)$ should be close to either 0 or 1. That’s why it’s convenient to use the sigmoid function.\n",
    "\n",
    "### Logistic Regression Methodology\n",
    "\n",
    "Again, from From [Mirko Stojiljković's Logistic Regression in Python](https://realpython.com/logistic-regression-python/):\n",
    "\n",
    "> Logistic regression is a linear classifier, so you’ll use a linear function $f(x) = b_0 +b_1x_1 + \\ldots+b_tx_t$, also called the **logit**. The variables $b_0, b_1, \\ldots, b_t$ are the estimators of the regression coefficients, which are also called the **predicted weights** or just **coefficients**.\n",
    "\n",
    "Sound familir??\n",
    "\n",
    "> The logistic regression function $p(x)$ is the sigmoid function of $f(x)$: \n",
    ">\n",
    "> $$ p(x) = \\frac{1}{1+exp^{-f(x)}} $$\n",
    ">\n",
    "> As such, it’s often close to either 0 or 1. The function  $p(x)$ is often interpreted as the predicted probability that the output for a given $x$ is equal to 1. Therefore, $1-p(x)$ is the probability that the output is 0.\n",
    ">\n",
    "> Logistic regression determines the best predicted weights $b_0, b_1, \\ldots, b_t$ such that the function $p(x)$ is as close as possible to all actual responses $y_i, i=1,\\ldots,n$ where $n$ is the number of observations. The process of calculating the best weights using available observations is called **model training** or **fitting**.\n",
    ">\n",
    ">To get the best weights, you usually maximize the **log-likelihood function (LLF)** for all observations  $i=1,\\ldots,n$. This method is called the **maximum likelihood estimation** and is represented by the equation \n",
    ">\n",
    "> $$ LLF = \\sum_{i}^{n} (y_ilog(p(x_i)) + (1-y_i)log(1-p(x_i)) $$\n",
    ">\n",
    "> When $y_i=0$, the LLF for the corresponding observation is equal to $log(1 - p(x_i)))$. If $p(x_i)$ is close to $y_i=0$, then $log(1 - p(x_i)))$ is close to 0. This is the result you want. \n",
    ">\n",
    "> If $p(x_i)$ is far from 0, then $log(1 - p(x_i))$ drops significantly. You don’t want that result because your goal is to obtain the maximum LLF. \n",
    ">\n",
    "> Similarly, when $y_i=1$, the LLF for that observation is $y_ilog(p_x)$. If $p(x_i)$ is close to $y_i=1$, then $log(p(x_i))$ is close to 0. If $p(x_i)$ is far from 1, then $log(p(x_i))$ is a large negative number.\n",
    ">\n",
    "> There’s one more important relationship between $p(x)$ and $f(x)$ which is that:\n",
    ">\n",
    ">$$ f(x) =log \\frac{(p_x)}{(1-p(x))} $$\n",
    ">\n",
    "> This equality explains why $f(x)$ is the [logit](https://en.wikipedia.org/wiki/Logit). It implies that $p(x)=0.5$ when $f(x)=0$ and that the predicted output is 1 if $f(x) > 0$ and 0 otherwise.\n",
    "\n",
    "### Single Variable Logistic Regression example\n",
    "\n",
    "Let's try to illustrate this with a small example using a single variable. If it helps to make it concrete, lets imagine that we are classifying movies based on the number of actors I know. I only wat to watch movies with 4 or more actors I know...\n",
    "\n",
    "Actors I know | Watch the movie?\n",
    "--------------|-----------------\n",
    "0  | No\n",
    "🧑 | No\n",
    "🧑👩🏿‍🚀 | No\n",
    "🧑👩🏿‍🚀👩🏾‍🏭 | No\n",
    "🧑👩🏿‍🚀👩🏾‍🏭👩🏼‍💼|Yes\n",
    "🧑👩🏿‍🚀👩🏾‍🏭👩👳🏻‍♂|Yes\n",
    "🧑👩🏿‍🚀👩🏾‍🏭👩👳🧔|Yes\n",
    "🧑👩🏿‍🚀👩🏾‍🏭👩👳🧔👩🏾‍🦰|Yes\n",
    "🧑👩🏿‍🚀👩🏾‍🏭👩👳🧔👩🏾‍🦰🧕🏽|Yes\n",
    "🧑👩🏿‍🚀👩🏾‍🏭👩👳🧔👩🏾‍🦰🧕🏽🤹🏼‍♀️|Yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's make some data to work with\n",
    "   # Reshape to column vector\n",
    "\n",
    "# Question what are these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of options for the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier. For this example, we will use the `liblinear` solver--the algorithm used in the optimization step. This is mostly for two reasons...\n",
    "1. Until recently this was the default (*lbfgs* is now default--[here is](https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451) a good article on the options) \n",
    "1. It produces some \"interesting\" results to discuss. Certainly look at the options and play with different solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a model using our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the model parameters\n",
    "\n",
    "print(\"Classes:\", model['logistic'].classes_)\n",
    "print(\"Intercept:\", model['logistic'].intercept_)\n",
    "print(\"Coefficients:\", model['logistic'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted y's for f(x)\n",
    "x_plot= np.linspace(0,10, num=100)\n",
    "y_fx = model['logistic'].intercept_[0] + ( model['logistic'].coef_[0][0] * x_plot )\n",
    "\n",
    "# Find where f(x)=0 so that we can plot that line\n",
    "fx_0 = -1 * model['logistic'].intercept_[0] / model['logistic'].coef_[0][0] \n",
    "\n",
    "# Use the logistic to get y's for p(x)\n",
    "y_px = 1 / (1 + np.exp(-1 * y_fx) )\n",
    "\n",
    "\n",
    "# Sorry, after fighting with plotnine for a bit I fell back to matplotlib for this plot\n",
    "plt.ylim([-0.1,1.1]) # Set Y-limits\n",
    "plt.grid(True)\n",
    "plt.scatter(range(10),y, color='green') # Plot the true y's in green\n",
    "#plt.plot(x_plot,y_fx, color='black', linestyle='dashed') # Plot f(x) line as black, dashed\n",
    "#plt.axvline(fx_0) # Plot line where f(x)=0, implying p(x)=0.5 Points to the left 0, right 1\n",
    "#plt.plot(x_plot, y_px, color='grey') # Plot the p(x) line as grey, solid\n",
    "plt.show()\n",
    "#print(fx_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the model do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output matrix above, each row corresponds to a single observation ($x$). The first column is the predicted probability of the output being the  $0^{th}$ class-- 0 or $(1-p(x))$. The second column is the predicted probability of class 1, $p(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The above example and image below come from the Real Python blog [*Logistic Regression in Python*](https://realpython.com/logistic-regression-python/).\n",
    "\n",
    "![](images/log-reg-5_real_python.png)\n",
    "\n",
    "AS in the graph we made, the green circles represent the actual responses ($y$) that are correctly predicted by the model. The red x shows the incorrect prediction for point 4. The solid line is the estimated logistic function $p(x)$, and the grey squares along that line are the predicted responses (second column in table). The black dashed line is the logit, $f(x)$.\n",
    "\n",
    "As the plot above and the `model.predict(x)` output show, the model correctly classifies nine of the 10 observations. As such, the accuracy is 9/10=0.9, which is available with the `model.score()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification metrics\n",
    "\n",
    "The `model.score()` tells us something about our model's performance, but not really everything we would like to know. \n",
    "\n",
    "Taking a different classification problem, let's look at a model that classifies images as either cat or not cat. Here are common terms used to describe classification success:\n",
    "\n",
    "![Cat classification success](images/classification_success.png)\n",
    "\n",
    "One way of gaining insight into our model is what is called a **confusion matrix**. This summarizes the true and false positive and negative counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "What kind of error is our model making?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Classification Metrics\n",
    "\n",
    "For the following, we will refer to the 0 class as negative and the 1 class as positive. \n",
    "\n",
    "**Precision**: The ability of the classifier not to label as positive (1) a sample that is negative (0). Out of those predicted positive, how many are actual positive.\n",
    "\n",
    "$$ Precision =  \\frac{True\\,Positive}{True\\,Positive\\,+\\,False\\,Positive} =  \\frac{True\\,Positive}{Total\\,Predicted\\,Positive} $$\n",
    "\n",
    "Precision is a good measure when the cost of False Positive is high. E.g. Email spam filter. If lots of email is falsely flagged as spam and discarded, you might miss important emails. Precision is also called the *positive predictive value*. There is also a *negative predictive value* that is the ratio of true negatives over the sum of true and false negatives.\n",
    "\n",
    "**Recall/Sensitivity**:  The ability of the classifier to find all the positive (1) samples. How many of the actually positives does the model label as positive.\n",
    "\n",
    "$$ Recall = \\frac{True\\,Positive}{True\\,Positive + False\\,Negative} =  \\frac{True\\,Positive}{Total\\,Actual\\,Positive} $$\n",
    "\n",
    "Recall is a good measure when the cost of False Negative is high. E.g. disease diagnosis. If a sick patient is predicted as not sick, the cost associated with a false negative is high. Recall is also called *sensitivity* or *True Positive Rate* (**TPR**).\n",
    "\n",
    "**Specificity**: The ability of the classifier to fine all the negative (0) samples. How many of the actually negatives does the model label as negative.\n",
    "\n",
    "$$ Specificity = \\frac{True\\,Negative}{True\\,Negative + False\\,Positive} = \\frac{True\\,Negative}{Total\\,Actual\\,Negative} $$\n",
    "\n",
    "Since we'll use this later, 1 - Specificity is known as the *False Positivity Rate (**FPR**)*.\n",
    "\n",
    "**F1-Score** also known as F-beta: A weighted harmonic mean of the precision and recall, where and F-beta score reaches its best value at 1 and worst score at 0. \n",
    "\n",
    "\n",
    "The F-beta score weighs recall more than precision by a factor of `beta`. `beta=1.0` means recall and precision are equally important. \n",
    "\n",
    "\n",
    "$$ F1 = 2\\frac{Precision\\,*\\,Recall}{Precision\\,+\\,Recall} $$\n",
    "\n",
    "**Support**: The number of samples in each class.\n",
    "  \n",
    "**Accuracy**: Total number of correct predictions over all predictions. Only accurate with balanced models (relatively equal frequency of 0/1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another set of classification metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression example\n",
    "\n",
    "For this example, let's predict if a patient will develop (1) diabetes or not (0). Note that this is a different dataset than we have been using. We have several variables that can be used for this prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine as pn\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "diabetes = pd.read_csv(\"data/diabetes.csv\")\n",
    "\n",
    "\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train/test data\n",
    "\n",
    "# Get the y values and drop from df\n",
    "\n",
    "# Split to 75% train, 25% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a model using our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "# Predict ys using model and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log odds\n",
    "\n",
    "One nice feature of logistic regression is that the coefficients are log odds or effect. We can get the $exp(coef\\_)$ and get the odds. In this example, for every unit increase in DiabetesPedigreeFunction, the odds of diabetes goes up by 1.8, or nearly doubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# odds = exp(coef_)\n",
    "\n",
    "\n",
    "# Create a dataframe and sort by odds column with highest at top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic (ROC) Curve and Area Under the Curve (AUC)\n",
    "\n",
    "Originating [rating radar operators in the 1940s](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#History), a ROC curve is a way to visualize the tradeoff between sensitivity and specificity.\n",
    "\n",
    "So far, we have been assuming anything below 0.5 is class 0 and anything about 0.5 is class 1. But that is one of any number of cutoffs and we could change the classification threshold.\n",
    "\n",
    "**What effect would having a lower threshold have on false positive and negative rates?**\n",
    "\n",
    "**When might you want to have more false positives? or more false positives?**\n",
    "\n",
    "Remember:\n",
    "* Recall = Sensitivity = TPR = true positives / total actual positives\n",
    "* Specificity = true negatives / total actual negatives\n",
    "  * 1 - Specificity = FPR\n",
    "\n",
    "The ROC curve plots these against each other at classification thresholds from 0 to 1. Often a line for a random model (randomly classify observations as 0/1--should be 50% accurate) is also added.\n",
    "\n",
    "The **Area Under the Curve (AUC)** is an aggregate measure of model performance across all classification thresholds. \n",
    "\n",
    "The higher the AUC, the better the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresh = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plot ROC w/ RocCurveDisplay\n",
    "#display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name='example estimator')\n",
    "#display.plot() \n",
    "\n",
    "# Plot ROC w/ plotnine\n",
    "df= pd.DataFrame(fpr,tpr)\n",
    "(pn.ggplot(df, pn.aes(x='fpr', y='tpr')) + pn.geom_line() +\n",
    " pn.geom_point(pn.aes(color=thresh)) +\n",
    " pn.scale_color_gradient(low='yellow', high='red', name='Threshold', limits=[0,1]) + \n",
    " pn.geom_text(pn.aes(x= 0.5, y= 0.1, label=auc)) + \n",
    " pn.geom_abline(intercept=0, slope=1, color='blue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
