{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10a52d8-ebda-49cb-9277-399711451c2c",
   "metadata": {},
   "source": [
    "# Transfer Learning for Pneumonia identification in Chest X-rays\n",
    "\n",
    "![Image from CheXNet](images/CheXNet_X-rays.png)\n",
    "\n",
    "When you start to learn something new, you do not start as a blank slate, knowing nothing about the world. Instead, you use your existing knowledge to apply that to learning the new information. This is the idea behind transfer learning. Rather than start a model from random states, can we use information from previous training and apply that to the new data/task.\n",
    "\n",
    "If you took the Nvidia DLI [*Getting started with Deep Learning*](https://courses.nvidia.com/courses/course-v1:DLI+S-FX-01+V1/about) course, there's a fun example used there. They start with the task of making a doggy door that lets dogs in and out of your house, and keeps cats inside. The example uses a pre-trained model. Early on, we talked about the compute and power that go into training some of the larger AI models, so making use of those pre-trained resources is both helpful and environmentally conscious. \n",
    "\n",
    "The example continues with a scenario where the Secret Service has learned of your amazing AI-powered doggy door, and contacts you with a request to make a special version for the White House that will only let Bo (President Obama's dog--the example is a bit older) in, but not other dogs. In this example, they use the VGG16 model and transfer learning to recognize Bo vs other dogs. The VGG16 model knows how to distinguish 1,000 categories of things, but can't distinguish Bo from other dogs. Rather than start from scratch, why not make use of the existing knowledge and improve on that?\n",
    "\n",
    "## Pre-trained models\n",
    "\n",
    "There are many sources of pre-trained models. Some are built into `tensorflow` and other frameworks, there are model zoos (like [modelzoo.co](https://modelzoo.co/), [Nvidia NGC](https://catalog.ngc.nvidia.com/models), [Hugging Face](https://huggingface.co/models)), and many are posted in GitHub or with the papers that describe them.\n",
    "\n",
    "You can export your own trained models using: `model.save('filename')`\n",
    "\n",
    "You can load models from disk with something like: `keras.models.load_model('filename')`\n",
    "\n",
    "Again, using pre-trained models when you can, not only saves time, but compute resources and energy. \n",
    "\n",
    "## Pre-trained models in medical imaging\n",
    "\n",
    "There has been a fair bit of debate about the best ways to make use of pre-trained models for medical imaging. In many ways, X-rays, CT-scans, skin lesions, etc differ from images of other things out in the world--the things that make up most of the datasets that these models are trained on. Satya Mallick has a great summary of considerations and methods in this article: [Transfer Learning for Medical Images](https://learnopencv.com/transfer-learning-for-medical-images/). In that article, Satya references this review: Morid, Borjali and Fiol. 2021. [A scoping review of transfer learning research on medical image analysis using ImageNet](https://doi.org/10.1016/j.compbiomed.2020.104115). *Computers in Biology and Medicine*. **128:** 104115.\n",
    "\n",
    "This figure summarizes papers that have used different pre-trained models for various medical imaging tasks.\n",
    "\n",
    "<figure>\n",
    "   <img align='center' src='images/Morid_etal_2021_Fig3.png' alt='Figure 3 from Morid et al (2021)'>\n",
    "   <figcaption halign='center'><strong>Figure 3 from Morid <i>et al</i> (2021):</strong> Frequency of studies using specific types of TL CNN models per image type.</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b93c2d6-1996-4fac-af43-10521d7b6775",
   "metadata": {},
   "source": [
    "## ImageNet\n",
    "\n",
    "Most of these models are trained on a benchmark set of images know as [ImageNet](https://www.image-net.org/). ![ImageNet logo](https://www.image-net.org/static_files/index_files/logo.jpg)\n",
    "\n",
    "ImageNet was developed by Fei-Fei Li and her research group and is one of the most important computer vision datasets. The most commonly used dataset has 1,000 image classes, though there are now datasets with [21,000 classes](https://arxiv.org/abs/2104.10972), bounding boxes for objects and additional attributes.\n",
    "\n",
    "Numerous privacy and ethical concerns have been raised about ImageNet: e.g. [this](https://analyticsindiamag.com/image-datasets-bias-privacy-mit/), [this](https://www.nature.com/articles/s42256-022-00442-2) and [this](https://visualai.princeton.edu/fcvd/assets/Denton_slides.pdf) . Privacy has been somewhat addressed by a blurred faces version of the dataset (see [this](https://venturebeat.com/2021/03/16/imagenet-creators-find-blurring-faces-for-privacy-has-a-minimal-impact-on-accuracy/)).\n",
    "\n",
    "## Models behave differently on medical images\n",
    "\n",
    "Some of the conclusions of Morid *et al.* (2021) are:\n",
    "* Model architectures that perform better on ImageNet don't necessarily perform better on medical images.\n",
    "  * Interestingly, some of the more complex, newer models perform worse than older models. \n",
    "  Satya notes:\n",
    "  > In other words, the newer architectures may be overfitting to ImageNet, which probably explains the popularity of older architectures in the medical domain. \n",
    "* Model family (e.g. DenseNet, VGGNet, ResNet) matters more for a task than size of model within a family (DenseNet 121, 169, 201).\n",
    "  * Model families are typically named with the number of layers\n",
    "* Using the model, with pre-trained weights from the ImageNet training is generally best\n",
    "* While it's common practice (and what we will do) to chop off the last layers, you can sometimes save computational time by truncating the models even more, chopping off whole blocks (we'll see the DenseNet has modular blocks repeated). \n",
    "  * I love Satya's term for this:\n",
    "  > Truncated architectures are DeepCakes, as in you can have your cake and eat it too!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef9c87-e900-4e77-b2e6-ecaaf91d89b4",
   "metadata": {},
   "source": [
    "## CheXNet\n",
    "\n",
    "The example we'll (somewhat) replicate in this exercise is from the paper by Rajpurkar *et al* ([2017: *CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning*](https://arxiv.org/abs/1711.05225)). The last author on this paper is another leader in AI, Andrew Ng. In this paper, they use the DenseNet121 model, with ImageNet weights, to do transfer learning to make a model that can interpret chest X-rays, specifically, identifying those with Pneumonia. The claim, both from the title and this tweet are that the model outperforms radiologists:\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Should radiologists be worried about their jobs? Breaking news: We can now diagnose pneumonia from chest X-rays better than radiologists. <a href=\"https://t.co/CjqbzSqwTx\">https://t.co/CjqbzSqwTx</a></p>&mdash; Andrew Ng (@AndrewYNg) <a href=\"https://twitter.com/AndrewYNg/status/930938692310482944?ref_src=twsrc%5Etfw\">November 15, 2017</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "\n",
    "There is an interesting review of the paper by Lauren Oakden-Rayner: [CheXNet: an in-depth review](https://laurenoakdenrayner.com/2018/01/24/chexnet-an-in-depth-review/). I encourage you to check it out!\n",
    "\n",
    "## What the CheXNet paper describes\n",
    "\n",
    "From the paper (p. 2):\n",
    "\n",
    "> ### 2.2. Model Architecture and Training\n",
    ">\n",
    "> CheXNet is a 121-layer Dense Convolutional Network (DenseNet) (Huang et al., 2016) trained on the ChestX-ray 14 dataset. DenseNets improve flow of information and gradients through the network, making the optimization of very deep networks tractable. We replace the final fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity. \n",
    ">\n",
    "> The weights of the network are initialized with weights from a model pretrained on ImageNet (Deng et al., 2009). The network is trained end-to-end using Adam with standard parameters (β1 = 0.9 and β2 = 0.999) (Kingma & Ba, 2014). We train the model using minibatches of size 16. We use an initial learning rate of 0.001 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss. \n",
    "\n",
    "\n",
    "## DenseNet\n",
    "\n",
    "DenseNet was described Huang *et al* [2016: *Densely Connected Convolutional Networks*](https://arxiv.org/abs/1608.06993) in and published as Huang *et al.* [2019](https://doi.org/10.1109/TPAMI.2019.2918284). Here's the abstract of the arxiv paper (my emphasis):\n",
    "> Recent work has shown that **convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output**. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which **connects each layer to every other layer** in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. **DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters**. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at [this https URL](https://github.com/liuzhuang13/DenseNet).\n",
    "\n",
    "And the figure describing the architecture:\n",
    "\n",
    "<figure>\n",
    "   <img align='center' src='images/Huang_etal_2017_DenseNet.png' alt='Figure 1 from Huang et al (2017)'>\n",
    "   <figcaption halign='center'><strong>Figure 1 from Huang <i>et al</i> (2017):</strong> A 5-layer dense block with a growth rate of k = 4. Each\n",
    "layer takes all preceding feature-maps as input.</figcaption>\n",
    "</figure>\n",
    "\n",
    "There blocks are then combined to make deeper models:\n",
    "\n",
    "<figure>\n",
    "   <img align='center' src='images/Huang_etal_2017_DenseNet_fig2.png' alt='Figure 2 from Huang et al (2017)'>\n",
    "   <figcaption halign='center'><strong>Figure 2 from Huang <i>et al</i> (2017):</strong>  A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change feature map sizes via convolution and pooling.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e0022-b332-4e0a-b876-c071fe17f764",
   "metadata": {},
   "source": [
    "## Transfer learning steps\n",
    "\n",
    "### 1. Select a model\n",
    "\n",
    "Again, DenseNet is one of many possible models, and within the DenseNet family, there are different sized models. For this, we will use the DenseNet121 model.\n",
    "\n",
    "### 2. Keep weights?\n",
    "\n",
    "It is possible to use the DenseNet model as a starting point and train that from scratch with our data. Doing this, we benefit from the work that went into designing the model architecture. However, this typically isn't the best approach. Unless we have large training datasets, it's unlikely that we will be as successful, especially with larger, deeper models, as training with millions of images in the commonly used datasets.\n",
    "\n",
    "For most transfer learning use cases, we keep the model weights that have been learned on a particular dataset. In our case, we will use the weights learned on ImageNet.\n",
    "\n",
    "### 3. Chop off the final layers\n",
    "\n",
    "The final layers of DenseNet121 are the classification layers for ImageNet that output probabilities that an input image belongs to one of 1,000 categories. We don't want that, we want a binary classification of Pneumonia, not Pneumonia. It's amazingly easy to do brain surgery on AI models, and chop off the final layers.\n",
    "\n",
    "### 4. Freeze the model weights\n",
    "\n",
    "Before we start training, we want to freeze the existing weights (from ImageNet) so that only our new layer we add in the next step are part of the initial training. We will look at optionally unfreezing these weights in fine tuning later.\n",
    "\n",
    "### 5. Add new layers for the task at hand\n",
    "\n",
    "To get a binary classification, we know we want a **sigmoid**. The paper again state: \"We replace the final fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity.\" (Rajpurkar et al 2017). The best I could interpret that was adding a dense layer and a sigmoid. I did find that I needed to flatten before the dense layer. The paper was implemented in PyTorch, and we're using Tensorflow/Keras, so it's a bit different in places...\n",
    "\n",
    "### 6. Train the model\n",
    "\n",
    "Now that we have our model we can train it using our chest X-ray data.\n",
    "\n",
    "### 7. Fine Tune the model\n",
    "\n",
    "As an optional step (one that we'll likely skip), we can unfreeze all the weights and fine tune the model using a low learning rate to increase performance even more.\n",
    "\n",
    "## Let's get going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373940be-8857-4c43-ba80-4e2274cf4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow.keras.applications.densenet import decode_predictions\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697c143-36bc-4e93-8923-20d112e04c09",
   "metadata": {},
   "source": [
    "## The DenseNet121 model\n",
    "\n",
    "First, let's take a quick look at the DenseNet121 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1dd5c-5067-4d24-862c-c88c757920fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DenseNet121 pre-trained on imagenet.\n",
    "\n",
    "model = tf.keras.applications.densenet.DenseNet121(\n",
    "    include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c5a5b-82ee-425e-9841-17ca9e87c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f5ba59-b66b-4b52-962f-001c55bc297f",
   "metadata": {},
   "source": [
    "That's a big model!\n",
    "\n",
    "## Test with an image\n",
    "\n",
    "Let's look at how the model does with an alligator image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8ac78-8551-4a1f-a926-60d8f5716cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtions to show the images, load and process them to the correct input shape and make a prediction\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    print(image.shape)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "def load_and_process_image(image_path):\n",
    "    # Print image's original shape, for reference\n",
    "    print('Original image shape: ', mpimg.imread(image_path).shape)\n",
    "    \n",
    "    # Load in the image with a target size of 224, 224\n",
    "    image = image_utils.load_img(image_path, target_size=(224, 224))\n",
    "    # Convert the image from a PIL format to a numpy array\n",
    "    image = image_utils.img_to_array(image)\n",
    "    # Add a dimension for number of images, in our case 1\n",
    "    image = image.reshape(1,224,224,3)\n",
    "    # Preprocess image to align with original ImageNet dataset\n",
    "    image = preprocess_input(image)\n",
    "    # Print image's shape after processing\n",
    "    print('Processed image shape: ', image.shape)\n",
    "    return image\n",
    "\n",
    "def densenet_prediction(image_path):\n",
    "    # Show image\n",
    "    show_image(image_path)\n",
    "    # Load and pre-process image\n",
    "    image = load_and_process_image(image_path)\n",
    "    # Make predictions\n",
    "    predictions = model.predict(image)\n",
    "    # Print predictions in readable form\n",
    "    print('Predicted:', decode_predictions(predictions, top=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa8e10-a77b-423e-849c-6ce329d2bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(\"images/American_Alligator.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430c6b6-e9c7-412f-bff4-51098de29319",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_image = load_and_process_image(\"images/American_Alligator.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea8636-c665-4e45-9798-3687b3fd66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_prediction(\"images/American_Alligator.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0ef35-782f-4a4c-97dd-bd50b8029050",
   "metadata": {},
   "source": [
    "### How does the model do on Chest X-rays?\n",
    "\n",
    "Just for fun, let's give it a Chest X-ray and see what it thinks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c6a28-0f34-4c69-8407-91c76a9dc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet_prediction(\"images/ChexNet_00007308_000.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7e294-7ab5-4bd0-9caa-c53f251932fa",
   "metadata": {},
   "source": [
    "Hmm...an Isopod???\n",
    "\n",
    "## Steps 1, 2 & 3: Load DenseNet without final layers and keep weights\n",
    "\n",
    "We can load DenseNet, this time, chopping off the final classification layers of the model. We can also specify we want to load the weights that were learned from ImageNet training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2dcea7-174b-4b8b-b843-8bfd456de75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DenseNet121 pre-trained on imagenet, this time exclude the top (last) layer\n",
    "\n",
    "model = tf.keras.applications.densenet.DenseNet121(\n",
    "    include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95792dfa-e904-4a3a-97a0-4c67fb88627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the model without the last layers, uncomment and run this...\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c09344-f6e7-4d29-a47b-166c92fa0a0f",
   "metadata": {},
   "source": [
    "## Step 4: Freeze the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208afb8e-41a4-4b15-acdf-e114ed2236e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2fcdb-4c35-49eb-b431-9df883b4dfa4",
   "metadata": {},
   "source": [
    "## Set a bunch of values for managing data and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84c724-d3e9-4143-a271-465cc7aece37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image dimensions, etc.\n",
    "resized_height = 224\n",
    "resized_width = 224\n",
    "num_channel = 3\n",
    "num_classes = 14\n",
    "batch_size = 128\n",
    "\n",
    "# Paths to images. These are too large to keep in the git repo. \n",
    "# Details of getting images ready are at the end of the notebook.\n",
    "base_dir = '/blue/zoo4926/share/ChestXray-NIHCC/'\n",
    "code_base ='Predict-Lung-Disease/'\n",
    "data_partitions_dir ='azure-share/chestxray/output/data_partitions/'\n",
    "image_dir= os.path.join(base_dir, 'images_Pneumonia_labeled_subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66449e42-6061-4688-8441-72e4d38ff2b8",
   "metadata": {},
   "source": [
    "## Step 5: Add new layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8fbd8-bac2-4a73-8a24-136d2766977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(resized_height, resized_width, num_channel))\n",
    "# Separately from setting trainable on the model, we set training to False \n",
    "x = model(inputs, training=False)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "flatten = keras.layers.Flatten()(x)\n",
    "dense = keras.layers.Dense(1)(flatten)\n",
    "outputs = Dense(1,activation='sigmoid')(dense)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cab792-a693-453d-bf36-6d9ec39d3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the densenet121 is compressed here and that these parameters are not trainable.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47729a6c-d8d7-483d-b209-3af270b099ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the new model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb0f1c-abe9-4d72-bcbe-42ec121f950c",
   "metadata": {},
   "source": [
    "## Step 0...lots of data prep\n",
    "\n",
    "I skipped this above, but yes, you still need to do data prep!\n",
    "\n",
    "Even with the work that went into making the images available, there is a bunch to do still. I've added more of the details below, but a few interesting things...\n",
    "\n",
    "First, I used scripts from [this repo](https://github.com/fatLime/Predict-Lung-Disease) to run the pre-processing that gets the image labels from a CSV file into a pickle file. \n",
    "\n",
    "Second, note that while we are using this for binary Pneumonia/Not-Pneumonia, there are actually 14 categories here. Also note that our \"not Pneumonia\" class is not \"healthy\"...there are a bunch of other disease states other than Pneumonia.\n",
    "\n",
    "### Preprocess the images\n",
    "\n",
    "```bash\n",
    "[magitz@c0907a-s17 Predict-Lung-Disease]$ export PYTHONPATH=/blue/zoo4926/share/ChestXray-NIHCC/Predict-Lung-Disease/src\n",
    "[magitz@c0907a-s17 Predict-Lung-Disease]$ python 000_preprocess.py                                                                 <azure_chestxray_utils.chestxray_consts object at 0x2b871942c910>\n",
    "/blue/zoo4926/share/ChestXray-NIHCC/Predict-Lung-Disease/azure-share/chestxray/data/ChestX-ray8\n",
    "/blue/zoo4926/share/ChestXray-NIHCC/Predict-Lung-Disease/azure-share/chestxray/output\n",
    "/blue/zoo4926/share/ChestXray-NIHCC/Predict-Lung-Disease/azure-share/chestxray/data/ChestX-ray8/ChestXray-NIHCC\n",
    "len of original patient id is 30805\n",
    "len of cleaned patient id is 30079\n",
    "len of unique patient id with annotated data 726\n",
    "len of patient id with annotated data 984\n",
    "first ten patient ids are [24303, 16035, 4967, 28624, 5378, 20335, 17069, 12271, 16975, 4469]\n",
    "train:21563 valid:3081 test:6161\n",
    "836\n",
    "[('No Finding', 60361), ('Infiltration', 9547), ('Atelectasis', 4215), ('Effusion', 3955), ('Nodule', 2705), ('Pneumothorax', 2194), ('Mass', 2139), ('Effusion|Infiltration', 1603), ('Atelectasis|Infiltration', 1350), ('Consolidation', 1310), ('Atelectasis|Effusion', 1165), ('Pleural_Thickening', 1126), ('Cardiomegaly', 1093), ('Emphysema', 892), ('Infiltration|Nodule', 829), ('Atelectasis|Effusion|Infiltration', 737), ('Fibrosis', 727), ('Edema', 628), ('Cardiomegaly|Effusion', 484), ('Consolidation|Infiltration', 441)]\n",
    "[('Infiltration|Cardiomegaly', 1), ('Cardiomegaly|Consolidation|Effusion|Pneumonia', 1), ('Emphysema|Infiltration|Nodule|Pleural_Thickening|Pneumothorax', 1), ('Atelectasis|Nodule|Pleural_Thickening|Pneumothorax', 1), ('Consolidation|Nodule|Pneumonia', 1), ('Atelectasis|Effusion|Emphysema|Pleural_Thickening|Pneumothorax', 1), ('Consolidation|Pneumonia|Mass', 1), ('Consolidation|Effusion|Emphysema', 1), ('Atelectasis|Hernia|Infiltration|Mass|Nodule|Pneumothorax', 1), ('Atelectasis|Consolidation|Mass|Pleural_Thickening|Pneumothorax', 1)]\n",
    "Atelectasis           11559\n",
    "Cardiomegaly           2776\n",
    "Consolidation          4667\n",
    "Edema                  2303\n",
    "Effusion              13317\n",
    "Emphysema              2516\n",
    "Fibrosis               1686\n",
    "Hernia                  227\n",
    "Infiltration          19894\n",
    "Mass                   5782\n",
    "No Finding            60361\n",
    "Nodule                 6331\n",
    "Pleural_Thickening     3385\n",
    "Pneumonia              1431\n",
    "Pneumothorax           5302\n",
    "dtype: int64\n",
    "100%|██████████████████████████████████████████████████████████████████████████████████████| 21563/21563 [00:13<00:00, 1648.38it/s]\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████████| 3081/3081 [00:01<00:00, 1678.42it/s]\n",
    "100%|████████████████████████████████████████████████████████████████████████████████████████| 6161/6161 [00:05<00:00, 1221.24it/s]\n",
    "train, valid, test image number is: 69217 9600 33303\n",
    "<class 'dict'>\n",
    "{'00024303_000.png': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), '00016035_001.png': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), '00016035_000.png': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), '00004967_000.png': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8), '00004967_001.png': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)}\n",
    "[magitz@c0907a-s17 Predict-Lung-Disease]$ \n",
    "```\n",
    "\n",
    "Also, there are 1,431 X-rays where Pneumonia is detected and 110,689 where it's not. That's a lot of images to process in class. I've made a subset to make things run faster (and we still get amazing results). But the full set of images are in these folders:\n",
    "\n",
    "Description | Path on HiPerGator\n",
    "------------|-------------------\n",
    "All images  | `/blue/zoo4926/share/ChestXray-NIHCC/images`\n",
    "All images in Pneumonia and Not_Pneumonia sub-folders | `/blue/zoo4926/share/ChestXray-NIHCC/images_Pneumonia_labeled`\n",
    "10% of the images randomly sub-sampled in labeled sub-folders | `/blue/zoo4926/share/ChestXray-NIHCC/images_Pneumonia_labeled_subset`\n",
    "All images in 15 categories | `/home/magitz/blue_zoo4926/share/ChestXray-NIHCC/images_14_cat`\n",
    "\n",
    "We'll be using the sub-sampled set. See the box above to change the path to the `image_dir` to select the full dataset.\n",
    "\n",
    "### Let's load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e733b4b-d69e-4b50-a94e-5fee359b3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note using rgb to get 3 color channels as needed as input for DenseNet\n",
    "train_images = image_dataset_from_directory(\n",
    "    image_dir, labels='inferred', label_mode='binary',\n",
    "     color_mode='rgb', batch_size=batch_size, image_size=(resized_height ,\n",
    "    resized_width ), shuffle=True, seed=42, validation_split=0.2, subset='training',\n",
    "    interpolation='bilinear', follow_links=False,\n",
    "    crop_to_aspect_ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fb741-809c-432b-bccc-68894a0c6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_images = image_dataset_from_directory(\n",
    "    image_dir, labels='inferred', label_mode='binary',\n",
    "     color_mode='rgb', batch_size=batch_size, image_size=(resized_height ,\n",
    "    resized_width ), shuffle=True, seed=42, validation_split=0.2, subset='validation',\n",
    "    interpolation='bilinear', follow_links=False,\n",
    "    crop_to_aspect_ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a971134-ce25-4a9b-ba06-e4b98f7aa71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at size of the dataset\n",
    "train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c89ef-7db5-4439-91ac-614d691801ef",
   "metadata": {},
   "source": [
    "### Look at some of the loaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779fc6a-3d64-448f-8d4d-d08eced8f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the images have been loaded with 3 color channels!\n",
    "class_names = train_images.class_names\n",
    "print(class_names)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_images.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    #plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "    print(images[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb98d9-c23b-49c8-86c5-6fb58dad8207",
   "metadata": {},
   "source": [
    "## Step 6. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b32875-5cdf-4320-94ab-27bc57055647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(train_images, batch_size=batch_size, epochs=2, verbose=1, validation_data=valid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a4ffa-9ef1-4bac-8e8e-624bee5de264",
   "metadata": {},
   "source": [
    "## Data prep stuff\n",
    "\n",
    "The cells below were used for preparing the data. They do not need to be run for the exercise, but I left them here for you to see what was done.\n",
    "\n",
    "### Load the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd91b70d-d811-447e-946e-310610af5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels from Pickel files generated by the 000_preprocess.py script.\n",
    "# The script is part of this repo: https://github.com/fatLime/Predict-Lung-Disease\n",
    "# For this class, it is located at /blue/zoo4926/share/ChestXray-NIHCC/Predict-Lung-Disease/000_preprocess.py\n",
    "\n",
    "label_path = os.path.join(base_dir, code_base, data_partitions_dir, 'labels14_unormalized_cleaned.pickle')\n",
    "\n",
    "partition_path = os.path.join(base_dir, code_base, data_partitions_dir, 'partition14_unormalized_cleaned.pickle')\n",
    "\n",
    "with open(label_path, 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "with open(partition_path, 'rb') as f:\n",
    "    partition = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40f6a6-f03d-44bb-93fa-fd16ab273820",
   "metadata": {},
   "source": [
    "### Process images into Pneumonia and Not_Pneumonia folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698b572-16db-43e3-a60e-25c08dff28fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folders for Pneumonia and Not_Pneumonia with the images\n",
    "# Note...there is likely an easier way to do this, pulling labels\n",
    "# from the pickle files or variable defines above, but...\n",
    "\n",
    "def process_Pneumonia(labels, in_path, out_base_path):\n",
    "    '''Takes a labels Pickle input and splits images into Pneumonia vs Not_Pneumonia'''\n",
    "    \n",
    "    pneumonia_path = os.path.join(out_base_path, 'Pneumonia')\n",
    "    not_path = os.path.join(out_base_path, 'Not_Pneumonia')\n",
    "    \n",
    "    print(f'Processing images in {in_path}')\n",
    "    print(f'Pneumonia is class 5 (index 6) in array, images with a 1 there will be put in: {pneumonia_path}')\n",
    "    print(f'Everything else will go in: {not_path}') # Note this is \n",
    "    \n",
    "    pneumonia_count = 0\n",
    "    not_count = 0\n",
    "    \n",
    "    for image_info in labels.items():\n",
    "        \n",
    "        if int(image_info[1][6]) == 1: #If Pneumonia, need to convert numpy.uint8 to int\n",
    "            shutil.copy(os.path.join(in_path, image_info[0]), os.path.join(out_base_path, 'Pneumonia', image_info[0]))\n",
    "            pneumonia_count+=1\n",
    "        else:\n",
    "            shutil.copy(os.path.join(in_path, image_info[0]), os.path.join(out_base_path, 'Not_Pneumonia', image_info[0]))\n",
    "            not_count+=1\n",
    "         \n",
    "    print('Done')\n",
    "    print(f'Found {pneumonia_count} Pneumonia images and {not_count} non-Pneumonia images.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2e0fb-4319-4db0-b516-2db34419e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function above with paths for output.\n",
    "img_dir = '/blue/zoo4926/share/ChestXray-NIHCC/images/'\n",
    "out_dir = '/blue/zoo4926/share/ChestXray-NIHCC/images_Pneumonia_labeled'\n",
    "process_Pneumonia(labels, img_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18faff97-fed7-4b5e-9e72-f855e659088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['00000061_003.png']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722848b8-06ba-4ba6-b01f-15d847669273",
   "metadata": {},
   "source": [
    "### Fix image errors\n",
    "\n",
    "Using the full dataset, there were two images that were porly formatted. These would throw the error:\n",
    "\n",
    " `W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG`\n",
    "\n",
    "To fix this, I found [this post](https://stackoverflow.com/questions/22745076/libpng-warning-iccp-known-incorrect-srgb-profile) that suggested identifying the problematic images with [`pngcrush`](https://pmt.sourceforge.io/pngcrush/) and fixing them with `mogrify` from [`imagemagick`](https://imagemagick.org/index.php).\n",
    "\n",
    "There were too many images in the main `images` folder to process all at once, so I used the folders split by diagnosis:\n",
    "\n",
    "```bash\n",
    "[magitz@c0903a-s35 pcr010813]$ for dir in `ls -d ../../share/ChestXray-NIHCC/images_14_cat/*`; do echo \"Working on $dir; ./pngcrush -warn -n ${dir}/*.png; done\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Atelectasis\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Cardiomegaly\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Consolidation\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Edema\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Effusion\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Emphysema\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Fibrosis\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Hernia\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Infiltration\n",
    "../../share/ChestXray-NIHCC/images_14_cat/Infiltration/00002846_012.png: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Mass\n",
    "../../share/ChestXray-NIHCC/images_14_cat/Mass/00000004_000.png: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Nodule\n",
    "../../share/ChestXray-NIHCC/images_14_cat/Nodule/00000004_000.png: iCCP: profile 'ICC Profile': 'GRAY': Gray color space not permitted on RGB PNG\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/No_Finding\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Pleural_Thickening\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Pneumonia\n",
    "Working on ../../share/ChestXray-NIHCC/images_14_cat/Pneumothorax\n",
    "[magitz@c0903a-s35 pcr010813]$\n",
    "```\n",
    "\n",
    "So, images `00002846_012.png` and `00000004_000.png` are the culpets. Fixing them with:\n",
    "   \n",
    "```bash\n",
    "[magitz@c0903a-s35 pcr010813]$ mogrify ../../share/ChestXray-NIHCC/images/00002846_012.png\n",
    "[magitz@c0903a-s35 pcr010813]$ mogrify ../../share/ChestXray-NIHCC/images/00000004_000.png\n",
    "```\n",
    "Seems to have fixed things..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d8ee6-72a4-4b0e-9241-010515e27f0e",
   "metadata": {},
   "source": [
    "### Process images into 14-desease categories (plus \"No_Finding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a3e18-5e63-4dbf-97de-fc80de41bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folders for Pneumonia and Not_Pneumonia with the images\n",
    "# Note...there is likely an easier way to do this, pulling labels\n",
    "# from the pickle files or variable defines above, but...\n",
    "\n",
    "def process_14(labels, in_path, out_base_path):\n",
    "    '''Takes a labels Pickle input and splits images into 14 diesease categories'''\n",
    " \n",
    "    # DISEASE_list is from: Predict-Lung-Disease/src/azure_chestxray_utils.py\n",
    "    DISEASE_list = ['Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                    'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "    disease_count = {}\n",
    "    \n",
    "    # Make the folders for each disease in base_out_path, setup counter\n",
    "    for disease in DISEASE_list:\n",
    "        os.mkdir(os.path.join(out_base_path, disease))\n",
    "        disease_count[disease]=0\n",
    "        \n",
    "    os.mkdir(os.path.join(out_base_path, 'No_Finding')) # Make the No_finding folder\n",
    "    disease_count['No_Finding']=0\n",
    "        \n",
    "    print(f'Processing images in {in_path}')\n",
    "    \n",
    "    for image_info in labels.items():\n",
    "        disease_index_number = 0\n",
    "        finding=0\n",
    "        # Go through each disease category and check if that disease is indicated\n",
    "        for disease in DISEASE_list:\n",
    "            if int(image_info[1][disease_index_number]) == 1: #If disease = 1, need to convert numpy.uint8 to int\n",
    "                shutil.copy(os.path.join(in_path, image_info[0]), os.path.join(out_base_path, disease, image_info[0]))\n",
    "                disease_count[disease]+=1\n",
    "                disease_index_number+=1 # Go to next disease index\n",
    "                finding+=1\n",
    "            else:\n",
    "                disease_index_number+=1 # Go to next disease index\n",
    "                \n",
    "        if finding == 0: # If No_finding\n",
    "            shutil.copy(os.path.join(in_path, image_info[0]), os.path.join(out_base_path, 'No_Finding', image_info[0]))\n",
    "            disease_count['No_Finding']+=1\n",
    "                \n",
    "          \n",
    "    print('Done')\n",
    "    \n",
    "    for disease in DISEASE_list:\n",
    "        print(f'Disease: {disease}:\\t {disease_count[disease]}')\n",
    "            \n",
    "    print(f\"No_Finding:\\t {disease_count['No_Finding']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98586aa0-c0fa-4317-83dc-cd20755c4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function above with paths for output.\n",
    "img_dir = '/blue/zoo4926/share/ChestXray-NIHCC/images/'\n",
    "out_dir = '/blue/zoo4926/share/ChestXray-NIHCC/images_14_cat'\n",
    "process_14(labels, img_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60242bd5-39aa-4154-9967-00780c8daa69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df3919fe-7f3a-4368-b675-c8103f795d60",
   "metadata": {},
   "source": [
    "### Randomly subsample the images to make datasets that can run in a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3eca8-7d79-4f6d-8ed3-99b62b168f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_images(in_base, out_base, fraction):\n",
    "    '''Takes an input folder and for each sub folder samples \n",
    "    a fraction of the files, putting the output into corresponding \n",
    "    out_base folders'''\n",
    "    \n",
    "    for in_base, dirs, files in os.walk(in_base):\n",
    "        for subdir in dirs:\n",
    "            print(f'Working in {os.path.join(in_base, subdir)}, copying {fraction} of the files to {os.path.join(out_base, subdir)}')\n",
    "            os.mkdir(os.path.join(out_base, subdir)) # Make the sub directory\n",
    "            for file in os.listdir(os.path.join(in_base, subdir)):\n",
    "                if random.random() < fraction:\n",
    "                    shutil.copy(os.path.join(in_base, subdir, file), os.path.join(out_base, subdir, file))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94fac23-2ef4-41ea-98c5-14e505b162f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/blue/zoo4926/share/ChestXray-NIHCC/images_Pneumonia_labeled'\n",
    "out_dir = '/blue/zoo4926/share/ChestXray-NIHCC/images_Pneumonia_labeled_subset'\n",
    "subset_images(img_dir,out_dir,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c45324-0473-42a6-bbc7-e93e06b790c9",
   "metadata": {},
   "source": [
    "### Let's see how a simple CNN does with these data\n",
    "\n",
    "Use the model from [17_CNNs_part2.ipynb](17_CNNs_part2.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cab95e-8db5-475b-bf33-2dddc0bfa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", \n",
    "                 input_shape=(resized_height, resized_width, 3)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model2.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model2.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(units=512, activation=\"relu\"))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ec0ef-9669-433f-a265-adf02c22b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42943bf-6a79-4316-9310-d5839c0f86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2204c2-53ec-4d55-bc62-f512afbe81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(train_images, batch_size=batch_size, epochs=20, verbose=1, validation_data=valid_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
